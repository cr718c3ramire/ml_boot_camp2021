{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. How to Fit a Logistic Regression Model\n",
    "\n",
    "We return to Maximum Likelihood Estimation from the Regression Notebook.\n",
    "\n",
    "Recall that in logistic regression we are interested in $P(y=1|X)$ let's call this $p(X;\\beta)$. In logistic regression we're modeling this as:\n",
    "$$\n",
    "p(X;\\beta) = \\frac{1}{1 + e^{-X\\beta}}.\n",
    "$$\n",
    "\n",
    "Now because our training data exists in a binary state we can't rely on the same procedure we did for linear regression. We instead use maximum likelihood estimation. We first must write out the likelihood function.\n",
    "\n",
    "First attempt to set up the $\\log$-likelihood for the logistic regression model, hint: we can think of $y_i$ as a bernouli random variable with probability parameter $p_i=p(X_i;\\beta)$.\n",
    "\n",
    "\n",
    "After you've accomplished that read through this reference starting at page 5 to see the derivation of the maximum likelihood estimate for logistic regression, <a href=\"https://cseweb.ucsd.edu/~elkan/250B/logreg.pdf\">https://cseweb.ucsd.edu/~elkan/250B/logreg.pdf</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Linear Discriminant Analysis\n",
    "\n",
    "In this \"question\" we'll learn about Linear Discriminant Analysis. With this technique we can extend logistic regression to multiple target classes as you'll do in the Applied Question below.\n",
    "\n",
    "Recall that the goal of logistic regression is to model:\n",
    "$$\n",
    "P(y=y^*|X=X^*).\n",
    "$$\n",
    "\n",
    "Suppose $y$ can be one of $k$ possible classes. Then for some input data, $X^*$, we have the following result dues to Baye's Theorem:\n",
    "\n",
    "$$\n",
    "P(y=i|X=X^*) = \\frac{P(\\{y=i\\}\\cap\\{X=X^*\\})}{P(X=X^*)} = \\frac{P(X=X^* | y=i) P(y=i)}{\\sum_{l=1}^k P(X=X^*|y=l) P(y=l)},\n",
    "$$\n",
    "\n",
    "for $i = 1,\\dots,k$.\n",
    "\n",
    "Now using the training data we can always estimate, $P(y=i)$ with the proportion of observations where $y=i$. The difficulty comes in estimating $P(X=X^* | y=i)$. A standard approach is to assume that this conditional probability follows a Gaussian distribution (note this is only appropriate for numeric predictors). In the case of a single prediction (i.e. $X$ is one dimensional), this means that:\n",
    "$$\n",
    "P(X=X^* | y=i) = \\frac{1}{\\sqrt{2\\pi\\sigma_i}}\\exp\\left(-\\frac{1}{2\\sigma_i^2}(X^* - \\mu_i)^2\\right),\n",
    "$$\n",
    "where $\\mu_i$ is the mean of $X$ conditional on $y=i$, and $\\sigma_i^2$ iis the variance of $X$ conditional on $y=i$.\n",
    "\n",
    "For simplicity we'd assume that $\\sigma_i^2 = \\sigma^2$ for all $i$, in which case the quantity we want to estimate, $P(y=i|X=X^*)$, is given by:\n",
    "\n",
    "$$\n",
    "P(y=i|X=X^*) = \\frac{\\pi_i \\frac{1}{\\sqrt{2\\pi} \\sigma} \\exp\\left(-\\frac{1}{2\\sigma^2} (X^*-\\mu_i)^2\\right)}{\\sum_{l=1}^k \\pi_l \\frac{1}{\\sqrt{2\\pi} \\sigma} \\exp\\left(-\\frac{1}{2\\sigma^2} (X^*-\\mu_l)^2\\right)},\n",
    "$$\n",
    "\n",
    "where $\\pi_i = P(y=i)$ for $i=1,\\dots,k$ and $\\pi$ without a subscript is just the mathematical constant.\n",
    "\n",
    "Linear discriminant analysis uses the training data to estimate the necessary parameters, $\\pi_i$, $\\mu_k$ and $\\sigma^2$, and then assigns each observation the class, $i$, for which $P(y=i|X=X^*)$ is largest.\n",
    "\n",
    "It can be shown that the appropriate class for a given $X^*$ is the class for which:\n",
    "\n",
    "$$\n",
    "\\delta_i(X^*) = X^* \\frac{\\mu_i}{\\sigma^2} - \\frac{\\mu_i^2}{2\\sigma^2} + \\log(\\pi_i).\n",
    "$$\n",
    "\n",
    "This $\\delta_i$ is referred to as the <i>discriminant</i>.\n",
    "\n",
    "#### Linear Discriminant Analysis on 1 Predictor for Binary Classification.\n",
    "\n",
    "<i>In the case for $k=2$ solve for the decision boundary</i>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension of LDA to multiple predictors, .i.e. an $n\\times m$ feature matrix, $X$\n",
    "\n",
    "For more than one predictor you're still trying to use the training data to estimate:\n",
    "\n",
    "$$\n",
    "P(y=i|X=X^*) = \\frac{P(\\{y=i\\}\\cap\\{X=X^*\\})}{P(X=X^*)} = \\frac{P(X=X^* | y=i) P(y=i)}{\\sum_{l=1}^k P(X=X^*|y=l) P(y=l)},\n",
    "$$\n",
    "\n",
    "However, the difference is that instead of assuming a normal distribution for $P(X=X^* | y=i)$ you assume a multivariate Gaussian with a fixed covariance matrix for all classes (i.e. the multivariate version of $\\sigma_i^2 = \\sigma^2$ for all $i$).\n",
    "\n",
    "### Quadratic Discriminant Analysis\n",
    "\n",
    "If you'd like to relax the assumption that each possible class for $y$ has the same variance (covariance matrix for multivariate models) then you can do this with Quadratic discriminant analysis. I'll leave it to you to find this, a good source is <a href=\"https://www.statlearning.com/\">An Introduction to Statistical Learning</a>.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Linear Discriminant Analysis\n",
    "\n",
    "In `sklearn` it can be implemented with `LinearDiscriminantAnalysis`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html\">https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html</a>.\n",
    "\n",
    "Use it to classify the toy data set below. Plot the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate([2*np.random.randn(50,2) + [2,2],\n",
    "                    2*np.random.randn(50,2) + [-1,-1],\n",
    "                    1*np.random.randn(50,2) + [-3,4]])\n",
    "\n",
    "y_train = np.concatenate([np.zeros(50),np.ones(50),2*np.ones(50)])\n",
    "\n",
    "\n",
    "X_test = np.concatenate([2*np.random.randn(50,2) + [2,2],\n",
    "                    2*np.random.randn(50,2) + [-1,-1],\n",
    "                    1*np.random.randn(50,2) + [-3,4]])\n",
    "\n",
    "y_test = np.concatenate([np.zeros(50),np.ones(50),2*np.ones(50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "plt.scatter(X_train[:50,0],X_train[:50,1],c='r',label=\"0\")\n",
    "plt.scatter(X_train[50:100,0],X_train[50:100,1],c='b',label=\"1\")\n",
    "plt.scatter(X_train[100:,0],X_train[100:,1],c='g',label=\"2\")\n",
    "\n",
    "plt.xlabel(\"$x_1$\",fontsize=14)\n",
    "plt.ylabel(\"$x_2$\",fontsize=14)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code or write here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code or write here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code or write here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2021.\n",
    "\n",
    "Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
