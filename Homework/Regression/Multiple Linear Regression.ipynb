{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "In this notebook are some exercises to gain more experience with the material presented in the Multiple Linear Regression lecture. You'll get some practice fitting models, and gain a stronger theoretical understanding of the technique as well. We'll also introduce some new important concepts that weren't explicitly covered in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages we'll use\n",
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import meshgrid\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Gradient Descent\n",
    "\n",
    "While we have the normal equation as the OLS estimate for $\\hat{\\beta}$ it is sometimes not ideal to to use that equation to find the estimate. This is because if there are too many features it can be computationally costly to perform the inverse operation.\n",
    "\n",
    "One alternative to the normal equation is to perform gradient descent.\n",
    "\n",
    "Let $\\ell(\\beta)$ denote a loss function. \n",
    "\n",
    "If we remember some Calculus III we'll remember that for a particular value of $\\beta$, say $\\beta^*$, the direction of greatest descent for $\\ell$ at $\\beta^*$, i.e. how to get to the minimum of $\\ell$ most quickly from $\\beta^*$, is the opposite direction of the gradient, $-\\nabla \\ell(\\beta^*)$. You can thus approach the value of $\\beta$ that minimizes $\\ell$ by iteratively updating $\\beta$ by moving in $\\alpha$ sized steps in the direction of greatest descent.\n",
    "\n",
    "Write out an algorithm (in mathematical symbols not code) that leverages the gradient of the loss function to find the optimal $\\hat{\\beta}$ for multiple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Statistical Significance of the Model\n",
    "\n",
    "One kind of explanatory modeling question we may be interested in is whether or not the model is statistically significant. Assuming that we have $m$ features this corresponds to the following hypothesis test:\n",
    "$$\n",
    "\\text{H}_0: \\beta_1 = \\beta_2 = \\dots = \\beta_m \\text{ vs.}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{H}_1: \\text{ at least one of } \\beta_i \\neq 0.\n",
    "$$\n",
    "This test allows you to say whether any of your predictors are significantly associated with the target $y$, when compared to the baseline model of $y=E(y)$. In non-statistical terms we're asking the question: \"Does my regression model contain at least one feature that helps explain what I see in the target variable?\"\n",
    "\n",
    "Let's see how we can perform this test using `statsmodels`.\n",
    "\n",
    "Suppose I fit a multiple linear regression for the following data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(100,2)\n",
    "y = 2 + 3*X[:,0] + np.random.randn(100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.907\n",
      "Model:                            OLS   Adj. R-squared:                  0.905\n",
      "Method:                 Least Squares   F-statistic:                     471.9\n",
      "Date:                Sat, 03 Oct 2020   Prob (F-statistic):           1.04e-50\n",
      "Time:                        11:32:53   Log-Likelihood:                -137.07\n",
      "No. Observations:                 100   AIC:                             280.1\n",
      "Df Residuals:                      97   BIC:                             288.0\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          1.9121      0.097     19.685      0.000       1.719       2.105\n",
      "x1             3.0210      0.099     30.621      0.000       2.825       3.217\n",
      "x2            -0.1406      0.090     -1.559      0.122      -0.319       0.038\n",
      "==============================================================================\n",
      "Omnibus:                        7.283   Durbin-Watson:                   1.892\n",
      "Prob(Omnibus):                  0.026   Jarque-Bera (JB):                7.623\n",
      "Skew:                          -0.458   Prob(JB):                       0.0221\n",
      "Kurtosis:                       3.996   Cond. No.                         1.20\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "fit = sm.OLS(y, sm.add_constant(X)).fit()\n",
    "\n",
    "print(fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the table.\n",
    "\n",
    "<img src = \"F_stat.png\" style=\"width:60%\"></img>\n",
    "\n",
    "The circled portion of the table is the $F$-statistic of the above hypothesis test and the $p$-value associated with said test. As we should expect here the $p$-value is incredibly low meaning that we would reject the null hypothesis in favor of the alternative.\n",
    "\n",
    "Now return to the beer data and use `statsmodels` to fit a multiple linear regression for the following model:\n",
    "$$\n",
    "\\text{IBU} = \\beta_0 + \\beta_1 \\text{ABV} + \\beta_2 \\text{Stout} + \\beta_3 \\text{Stout} \\times \\text{ABV}.\n",
    "$$\n",
    "\n",
    "Perform the above hypothesis test and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. The General Linear $F$-Test or Partial $F$-Test\n",
    "\n",
    "The statistical test we showed in problem 2. is not all that useful for determining if specific variables are useful for explaining the variance we see in the target data.\n",
    "\n",
    "You can always perform a hypothesis test for a single coefficient ($\\text{H}_0: \\beta_i = 0$ vs. $\\text{H}_1: \\beta_i \\neq 0$) by examining the confidence interval associated with that coefficient. If the interval contains $0$ you know that the variable is not statistically significant at that confidence level (standard level is $95\\%$).\n",
    "\n",
    "Another test you may wish to perform is the General Linear $F$-Test (also called partial $F$-Test). In this test you are asking the question \"Is my target related to this collection of variables?\". For example suppose you're interested in testing $\\beta_3$, $\\beta_4$, and $\\beta_5$:\n",
    "$$\n",
    "\\text{H}_0: \\beta_3 = \\beta_4 = \\beta_5 = 0 \\text{ vs.}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{H}_1: \\text{ at least one of } \\beta_i, \\ i = 3,4,5 \\text{ are nonzero}\n",
    "$$\n",
    "\n",
    "This test is useful when you're interested in the effects of a categorical variable with more than one category. In that setting you can't just test if one of the categories has a significant effect, you must test all categories concurrently. Thus the confidence interval procedure mentioned above doesn't work.\n",
    "\n",
    "In order to perform this test you need two models, a <i>full model</i> that must at least contain the variables you're interested in testing, and a <i>reduced model</i> that is the full model with the variables you're interested in removed.\n",
    "\n",
    "We'll now show how to do this test with `statsmodels`. \n",
    "\n",
    "For this example the full model is:\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 \\text{a} + \\beta_2 \\text{b}.\n",
    "$$\n",
    "\n",
    "The reduced model is:\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I create some fake data\n",
    "X = np.zeros((600,2))\n",
    "np.random.seed(440)\n",
    "\n",
    "# The first column is a continuous feature\n",
    "X[:,0] = 2*np.random.randn(600)-1\n",
    "\n",
    "# The second is categorical\n",
    "X[:200,1] = 1\n",
    "X[200:400,1] = 2\n",
    "X[400:,1] = 3\n",
    "\n",
    "# y = 1 + 2x_1 + - 2*1_{x_2 == 1} + epsilon\n",
    "y = 1 + 2*X[:,0] + np.random.randn(600)\n",
    "y[X[:,1] == 1] = y[X[:,1] == 1] - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I put it in a dataframe\n",
    "df = pd.DataFrame({'y':y,'x1':X[:,0],'x2':X[:,1]})\n",
    "\n",
    "df.loc[df.x2 == 1,'x2'] = 'a'\n",
    "df.loc[df.x2 == 2,'x2'] = 'b'\n",
    "df.loc[df.x2 == 3,'x2'] = 'c'\n",
    "\n",
    "# Make dummy variables\n",
    "df[['a','b']] = pd.get_dummies(df['x2'])[['a','b']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F-statistic is [[244.13578899]] which has an associate p-value of 3.566476961458313e-78\n"
     ]
    }
   ],
   "source": [
    "# First you fit the full model\n",
    "fit = sm.OLS(df['y'],sm.add_constant(df[['x1','a','b']])).fit()\n",
    "\n",
    "# Then we type out the specific hypothesis we're\n",
    "# testing for the reduced model as a string\n",
    "hypotheses = 'a=b=0'\n",
    "\n",
    "# You can then call the f_test with that hypothesis\n",
    "f_test = fit.f_test(hypotheses)\n",
    "\n",
    "print(\"The F-statistic is\",f_test.fvalue, \n",
    "      \"which has an associate p-value of\", f_test.pvalue)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of this test inform us that there is very strong statistical evidence of the categorical variable having an effect on the target `y`. Thus it would be wise to leave it in the model for explanatory purposes.\n",
    "\n",
    "Return to the `carseats` data set and build a full model that includes the `ShelveLoc` variable. Then perform a partial $F$-test to see if there is evidence that shelve location has an effect on `Sales`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Sum of Squares Table\n",
    "\n",
    "Recall that the goal of explanatory modeling is to help explain the target data. This means trying to explain the variance of the target data.\n",
    "\n",
    "One way we can examine this is by looking at the <i>Sum of Squares Table</i>.\n",
    "\n",
    "Recall the variance of $y$ is how much it deviates from $\\overline{y}$, i.e.\n",
    "$$\n",
    "\\sum_{i=1}^n \\left(y_i - \\overline{y}\\right)^2 = \\text{SST},\n",
    "$$\n",
    "the total sum of squares, it can be shown that:\n",
    "$$\n",
    "\\text{SST} = \\sum_{i=1}^n \\left(\\hat{y}_i - \\overline{y} \\right)^2 + \\sum_{i=1}^n\\left(\\hat{y}_i - y_i\\right)^2 = \\text{SSM} + \\text{SSR},\n",
    "$$\n",
    "where $\\text{SSM}$ denotes the model sum of squares and $\\text{SSR}$ denotes the residual sum of squares.\n",
    "\n",
    "Note the $\\text{SSR}$ is $n \\text{MSE}$, and thus estimates $\\sigma^2$. The $\\text{SSM}$ term estimates the variance in $y$ that is explained by the model.\n",
    "\n",
    "Further, for multiple linear regression $\\text{SSM}$ can be broken down into contributions to the variance from each individual variable.\n",
    "\n",
    "Let's demonstrate how to get the sum of squares table using `statsmodels`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I create some fake data\n",
    "np.random.seed(440)\n",
    "X = np.zeros((600,2))\n",
    "\n",
    "# The first column is a continuous feature\n",
    "X[:,0] = 2*np.random.randn(600)-1\n",
    "\n",
    "# The second is categorical\n",
    "X[:200,1] = 1\n",
    "X[200:400,1] = 2\n",
    "X[400:,1] = 3\n",
    "\n",
    "# y = 1 + 2x_1 + - 2*1_{x_2 == 1} + epsilon\n",
    "y = 1 + 2*X[:,0] + np.random.randn(600)\n",
    "y[X[:,1] == 1] = y[X[:,1] == 1] - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I put it in a dataframe\n",
    "df = pd.DataFrame({'y':y,'x1':X[:,0],'x2':X[:,1]})\n",
    "\n",
    "df.loc[df.x2 == 1,'x2'] = 'a'\n",
    "df.loc[df.x2 == 2,'x2'] = 'b'\n",
    "df.loc[df.x2 == 3,'x2'] = 'c'\n",
    "\n",
    "# Make dummy variables\n",
    "df[['a','b']] = pd.get_dummies(df['x2'])[['a','b']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>df</th>\n",
       "      <th>sum_sq</th>\n",
       "      <th>mean_sq</th>\n",
       "      <th>F</th>\n",
       "      <th>PR(&gt;F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9821.083158</td>\n",
       "      <td>9821.083158</td>\n",
       "      <td>9566.082414</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1.0</td>\n",
       "      <td>500.690058</td>\n",
       "      <td>500.690058</td>\n",
       "      <td>487.689829</td>\n",
       "      <td>2.033895e-79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.597257</td>\n",
       "      <td>0.597257</td>\n",
       "      <td>0.581749</td>\n",
       "      <td>4.459292e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Residual</th>\n",
       "      <td>596.0</td>\n",
       "      <td>611.887428</td>\n",
       "      <td>1.026657</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             df       sum_sq      mean_sq            F        PR(>F)\n",
       "x1          1.0  9821.083158  9821.083158  9566.082414  0.000000e+00\n",
       "a           1.0   500.690058   500.690058   487.689829  2.033895e-79\n",
       "b           1.0     0.597257     0.597257     0.581749  4.459292e-01\n",
       "Residual  596.0   611.887428     1.026657          NaN           NaN"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First you fit the model\n",
    "# note that we use ols instead of sm.OLS\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# for ols you write the regression formula\n",
    "# as a string\n",
    "fit = ols('y ~ x1 + a + b', df).fit()\n",
    "\n",
    "\n",
    "sm.stats.anova_lm(fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above table you can sum up the `sum_sq` column to get $\\text{SST}$. Each entry in the `sum_sq` column is that variable's (or the residual's) contribution to the $\\text{SST}$.\n",
    "\n",
    "<b>Note: Sums of squares are quite susceptible to the scale of the data. So a variable with a very large scale (say the 1000s) may seem more important than another variable with a very small scale (say 1/10s). Thus it is important to scale your data before fitting the model.</b>\n",
    "\n",
    "Return to the `beer` data, then create the sum of squares table for the following model:\n",
    "$$\n",
    "\\text{IBU} = \\beta_0 + \\beta_1 \\text{ABV} + \\beta_2 \\text{Stout} + \\beta_3 \\text{ABV} \\times \\text{Stout} + \\epsilon.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    IBU   R-squared:                       0.493\n",
      "Model:                            OLS   Adj. R-squared:                  0.489\n",
      "Method:                 Least Squares   F-statistic:                     111.3\n",
      "Date:                Sun, 25 Apr 2021   Prob (F-statistic):           2.39e-50\n",
      "Time:                        16:49:34   Log-Likelihood:                -1478.1\n",
      "No. Observations:                 347   AIC:                             2964.\n",
      "Df Residuals:                     343   BIC:                             2980.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     10.4825      6.010      1.744      0.082      -1.339      22.304\n",
      "ABV            7.9806      0.816      9.775      0.000       6.375       9.586\n",
      "Stout         -2.2906      7.424     -0.309      0.758     -16.893      12.312\n",
      "Stout:ABV     -3.5838      0.959     -3.736      0.000      -5.471      -1.697\n",
      "==============================================================================\n",
      "Omnibus:                      315.369   Durbin-Watson:                   2.184\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            18466.780\n",
      "Skew:                           3.405   Prob(JB):                         0.00\n",
      "Kurtosis:                      38.084   Cond. No.                         91.7\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "SUM OF SQUARES TABLE\n",
      "              df         sum_sq       mean_sq           F        PR(>F)\n",
      "ABV          1.0   26296.501488  26296.501488   88.578529  7.299234e-19\n",
      "Stout        1.0   68707.203261  68707.203261  231.436983  2.644882e-40\n",
      "Stout:ABV    1.0    4143.705912   4143.705912   13.957879  2.188972e-04\n",
      "Residual   343.0  101827.159944    296.872186         NaN           NaN\n"
     ]
    }
   ],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Interpreting Interaction Terms.\n",
    "\n",
    "Look at the model you just fit for the `beer` data. Try to interpret the estimates of the various coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Model Selection Algorithms\n",
    "\n",
    "Here we'll describe two additional model selection algorithms.\n",
    "\n",
    "##### Forwards Selection\n",
    "\n",
    "Say you have $m$ predictors $X_1,\\dots,X_m$, and a target $y$. \n",
    "\n",
    "Starting with an empty model you build $m$ simple linear regression models and then choose the one with lowest testing error (for instance by looking at the average cv error). Call this model $1$. \n",
    "\n",
    "Take model $1$ and go through the remaining $m-1$ features and add them one at a time to model $1$. This will give you $m-1$ two feature models. Look at the one with lowest testing error, call it model $2$. If model $2$ has lower testing error than model $1$ continue in this way and look at the remaining $m-2$ predictors. If model $1$ has the lower testing error you stop and model $1$ is the model you choose.\n",
    "\n",
    "You continue until you either find a model with lowest testing error (for example if model $3$ had lower testing error than model $4$ you chose model $3$), or until you have built the model regressing $y$ on all of $X_1,\\dots,X_m$.\n",
    "\n",
    "##### Backwards Selection\n",
    "\n",
    "This algorithm is sort of the opposite of forwards selection.\n",
    "\n",
    "Again say you have $m$ predictors $X_1,\\dots,X_m$ and a target $y$.\n",
    "\n",
    "Starting with the model regressing $y$ on all of $X_1,\\dots,X_m$, remove each of the $X_i$ predictors one at a time, regressing $y$ on the remaining $m-1$ features. If one of these models has lower testing error than the full model take it and call it model $1$. If none of those models has lower testing error than the full model stick with the full model.\n",
    "\n",
    "Take model $1$ and remove each of the $m-1$ predictors one at a time, regressing $y$ on the remaining $m-2$ features. If one of those models has lower testing error than model $1$ take it and call it model $2$. If none of those models has lower testing error than model $1$ stick with model $1$.\n",
    "\n",
    "Continue in this way until you have a reduced model with lowest testing error, or until you end up with the model with no predictors, i.e. $y = E(y)$.\n",
    "\n",
    "###### Greedy Algorithms\n",
    "\n",
    "These are both <i>greedy algorithms</i> because at each step you take the move that benefits you the most in the moment, but you don't explore suboptimal paths that may be better in the long run. While you may not get the best model, you're willing to go with a model that is close to correct in a faster time. Both of these algorithms at worst require fitting $m!$ models as opposed to the $2^m$ models required for the brute force approach.\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "Choose one of either forwards or backwards selection and program the algorithm to build a model to predict `Sales` from the `Advertising` data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Gradient Descent in Action\n",
    "\n",
    "Using your answer to question 1. under Theoretical Questions use `numpy` to perform gradient descent to fit the multiple linear regression model for the following data. <i>Note: If you didn't do that question, you can wait to try this one until after I provide solutions :)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare the output of your code to the model you get using `SGDRegressor`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html\">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Build the Best Predictive Model You Can\n",
    "\n",
    "Return to the data set from `PredictiveModelingAssessmentData.csv`. Use cross-validation to build the best predictive model you can to predict the `y` variable. As a hint theoretically the best you can do is a mean root mean square error of $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2021.\n",
    "\n",
    "Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
