{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bias-Variance Tradeoff\n",
    "\n",
    "In continuing with our work on regression for predictive modeling we'll discuss the problem of overfitting and introduce a key concept in all predictive modeling, the bias-variance tradeoff.\n",
    "\n",
    "## What We'll Accomplish in This Notebook\n",
    "\n",
    "We will:\n",
    "- Discuss ways you can overfit regression models\n",
    "- Derive the bias-variance tradeoff formula\n",
    "- See the tradeoff in action with a synthetic data set\n",
    "\n",
    "Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages we'll use\n",
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import meshgrid\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "A big problem that can occur when building a predictive modeling is overfitting to the training data. When you can perform cross-validation this is somewhat mitigated. However, due to various limitations it may not be possible to perform cross-validation for model selection.\n",
    "\n",
    "We'll first show you two ways in which overfitting can occur, then we'll show the theoretical reasons that overfitting occurs.\n",
    "\n",
    "### You Code\n",
    "\n",
    "\n",
    "### Including Too Many Features\n",
    "\n",
    "One way to overfit a model is to include too many features into the model.\n",
    "\n",
    "You'll work through an extreme example of this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate some data.\n",
    "np.random.seed(440)\n",
    "\n",
    "X_train = np.random.randint(1,100,(20,4))\n",
    "\n",
    "y_train = np.random.randint(1,100,20)\n",
    "\n",
    "\n",
    "X_test = np.random.randint(1,100,(20,4))\n",
    "\n",
    "y_test = np.random.randint(1,100,20)\n",
    "\n",
    "print(\"X=\",X_train)\n",
    "print()\n",
    "print(\"y=\",y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now iteratively add in each column of $X$ in models predicting $y$, i.e. fit all of the following models:\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 X_1 + \\epsilon\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\epsilon\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\beta_4 X_4 + \\epsilon\n",
    "$$\n",
    "\n",
    "Print out the training MSE for each model, at the same time print out the testing MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import LinearRegression here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this function to quickly get the MSE\n",
    "def get_mse(model,X,y):\n",
    "    pred = model.predict(X)\n",
    "    return np.sum(np.power(pred-y,2))/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit your models and print out the Training and Testing MSE here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your notes in this markdown chunk\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Degree Polynomials\n",
    "\n",
    "Another common way to display overfitting the data is to add too high a degree of polynomials to the data. We'll construct an example to show the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use these as your train and test data\n",
    "x_train = np.linspace(-1.5,1.5,500)\n",
    "x_test = np.linspace(-1.5,1.5,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## making y_test and y_train\n",
    "y_train = .1 + .2*x_train**2 + .5*x_train**4 - .5*x_train**5 + .7*np.random.randn(500)\n",
    "y_test = .1 + .2*x_test**2 + .5*x_test**4 - .5*x_test**5 + .7*np.random.randn(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot y_train against x_train\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.scatter(x_train,y_train)\n",
    "\n",
    "plt.xlabel(\"x\",fontsize=14)\n",
    "plt.ylabel(\"y\",fontsize=14)\n",
    "\n",
    "plt.title(\"Training Data\", fontsize=16)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import things we'll need\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a loop from i = 1 to 31, skipping the even numbers,\n",
    "\n",
    "# for each i make a pipeline that transforms x\n",
    "# into a polynomial array of degree i\n",
    "# then fit a regression of y on the powers of x\n",
    "# record both the training set mse and the test set mse\n",
    "\n",
    "train_mse = []\n",
    "test_mse = []\n",
    "\n",
    "for i in range(1,33,2):\n",
    "    pipe = Pipeline([('poly',PolynomialFeatures(degree=i,include_bias=False)),\n",
    "                    ('reg',LinearRegression(copy_X = True))])\n",
    "    \n",
    "    pipe.fit(x_train.reshape(-1,1),y_train)\n",
    "\n",
    "    train_mse.append(get_mse(pipe,x_train.reshape(-1,1),y_train))\n",
    "    test_mse.append(get_mse(pipe,x_test.reshape(-1,1),y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the degree vs the corresponding training and test mses\n",
    "# Label them so you know which is which.\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.plot(range(1,33,2),train_mse,\n",
    "         'r-o', label = \"Training MSE\")\n",
    "plt.plot(range(1,33,2),test_mse,\n",
    "            'b-*',label=\"Testing MSE\")\n",
    "\n",
    "plt.xlabel(\"Degree of Model\", fontsize=16)\n",
    "plt.ylabel(\"MSE\", fontsize=16)\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What did you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to write down what you observed through this exercise in this markdown chunk.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bias-Variance Trade-Off\n",
    "\n",
    "Let's return to our statistical learning framework from Notebook 1. Remember we try to fit the following model:\n",
    "$$\n",
    "y = f(X) + \\epsilon,\n",
    "$$\n",
    "where $f$ is some function.\n",
    "\n",
    "When we fit the model we produce some estimate of $f$ called $\\hat{f}$. \n",
    "\n",
    "As we've discussed we're interested in the generalization error of our algorithm, which so far has been the test MSE. Like with CV when we consider the generalization error we look at the expected value of the squared difference between $y$ and $\\hat{y}$. If we let $y_0$ and $X_0$ denote a single test set, we can write this mathematically as:\n",
    "$$\n",
    "E\\left[ \\left( y_0 - \\hat{y_0} \\right)^2 \\right]= E\\left[ \\left( y_0 - \\hat{f}(X_0) \\right)^2 \\right] = E \\left[ \\left( f(X_0) - \\hat{f}(X_0) + \\epsilon   \\right)^2 \\right],\n",
    "$$\n",
    "where all expectations are taken over the training space.\n",
    "\n",
    "With a little manipulation you can rewrite this as:\n",
    "$$\n",
    "\\text{Var}\\left(\\hat{f}(X_0)\\right) + \\left[ \\text{Bias}\\left( \\hat{f}(X_0) \\right) \\right]^2 + \\text{Var}(\\epsilon)\n",
    "$$\n",
    "$$\n",
    "= \\text{Variance of }\\hat{f} + \\text{Bias squared of }\\hat{f} + \\text{irreducible error}.\n",
    "$$\n",
    "\n",
    "If $\\text{Bias}$ is unfamiliar to you then, $\\text{Bias}\\left(\\hat{f}(X)\\right) = E\\left( f(X) - \\hat{f}(X) \\right)$. One way to think about it is how far on average is the estimator $\\hat{f}$ from the thing it is estimating, $f$.\n",
    "\n",
    "Since $\\text{Var}$ and $\\text{Bias}^2$ are both nonnegative, the best we can do is produce an algorithm with irreducible error $\\text{Var}(\\epsilon)$. This means we can reduce our generalization error by reducing our $\\text{Bias}$ or our $\\text{Var}$. However, it is often not possible to reduce both simultaneously. Many times lowering an algorithm's $\\text{Bias}$ leads to an increase in its $\\text{Var}$. Typically high bias indicates underfitting the data, while high variance means overfitting.\n",
    "\n",
    "We'll now show this in action with linear regression.\n",
    "\n",
    "### Bias-Variance Trade-Off with Linear Regression\n",
    "\n",
    "Let's build some toy data to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3,3,500)\n",
    "y = x*(x-1) + 1.2*np.random.randn(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,12))\n",
    "\n",
    "plt.plot(x, x*(x-1),'k', label = \"True Relationship, $f$\", alpha = .7)\n",
    "plt.scatter(x, y, label = \"Training Data\")\n",
    "\n",
    "plt.ylabel(\"y\", fontsize=16)\n",
    "plt.xlabel(\"x\", fontsize=16)\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear regression model with high bias but low variance would be to use no predictors and just take the mean observed $y$ value. This would be way underfitting the data, which has a clear pattern. It is high bias because we are far from the true relationship between $y$ and $x$. It is low variance because with a large enough sample the law of large numbers tells us the sample mean should be close to $E(y)$. So as long as our training sample is large enough then we won't vary much over different samples.\n",
    "\n",
    "A linear regression model with low bias but high variance would be to use a high degree polynomial of $x$. This is low bias because a high degree polynomial will more closely fit the true relationship. It will have high variance because as the degree of the fitting polynomial increases the more likely it is that the regression polynomial will attempt to fit all of the training points perfectly, meaning that you will get wildly different fits with each training set.\n",
    "\n",
    "A goldilocks, not too cold not too hot, model would be one with a low degree polynomial. For this particular problem it would likely be a parabola model, but could be a slightly higher degree in practice.\n",
    "\n",
    "In the following code block we fit three models five times. The first model just takes the mean of the training $y$ values. The second model is the \"Goldilocks\" model where we fit a parabola to the data. The final model is the high variance model where we fit a degree $20$ polynomial to the data. We then plot the predicted values for all 15 total model fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll plot 5 instances of all 3 models\n",
    "fig,ax = plt.subplots(1,3,figsize = (16,8), sharex = True, sharey = True)\n",
    "bias = []\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    # First generate a random training set\n",
    "    x = np.linspace(-3,3,100)\n",
    "    y = x*(x-1) + 1.2*np.random.randn(100)\n",
    "\n",
    "    # Now fit the high variance model\n",
    "    high_deg_pipe = Pipeline([('poly',PolynomialFeatures(20)),\n",
    "                             ('reg', LinearRegression(copy_X = True))])\n",
    "\n",
    "    high_deg_pipe.fit(x.reshape(-1,1),y)\n",
    "\n",
    "    high_deg_line = high_deg_pipe.predict(x.reshape(-1,1))\n",
    "\n",
    "\n",
    "\n",
    "    # Now fit the \"goldilocks model\", the true correct degree\n",
    "    pipe = Pipeline([('poly',PolynomialFeatures(2)),\n",
    "                    ('reg', LinearRegression(copy_X = True))])\n",
    "    pipe.fit(x.reshape(-1,1),y)\n",
    "    pipe_line = pipe.predict(x.reshape(-1,1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Plot High Bias low Variance plot\n",
    "    ax[0].plot(x, np.mean(y)*np.ones(len(x)), alpha = 1)\n",
    "    ax[0].set_title(\"High Bias, Low Variance\",fontsize=18)\n",
    "    ax[0].set_ylabel(\"y\", fontsize=16)\n",
    "    ax[0].set_xlabel(\"x\", fontsize=16)\n",
    "\n",
    "\n",
    "    # Plot Goldilocks model\n",
    "    ax[1].plot(x, pipe_line, alpha = 1)\n",
    "    ax[1].set_title(\"Just Right Model\",fontsize=18)\n",
    "    ax[1].set_xlabel(\"x\", fontsize=16)\n",
    "\n",
    "    # Plot High Va model\n",
    "    ax[2].plot(x, high_deg_line, alpha = 1)\n",
    "    ax[2].set_title(\"Low Bias, High Variance\",fontsize=18)\n",
    "    ax[2].set_xlabel(\"x\", fontsize=16)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions?\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Often the best model is the one that finds a balance between the Bias and the Variance. If you Google bias-variance trade-off, you'll see a picture like this along with links to tons of poorly written blog posts.\n",
    "<img src=\"biasvariance.png\"></img>\n",
    "\n",
    "This picture shows that the place where the minimal generalization error occurs is near to where the Bias equals the Variance. Let's picture the average testing error as a function of different degrees for this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a single test set\n",
    "x_test = np.linspace(-3,3,100)\n",
    "y_test = x_test*(x_test-1) + 1.2*np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_errors = np.zeros((10,len(range(1,41))))\n",
    "\n",
    "# We'll generate 10 training sets\n",
    "for i in range(10):\n",
    "    x = np.linspace(-3,3,100)\n",
    "    y = x*(x-1) + 1.2*np.random.randn(100)\n",
    "\n",
    "    for j in range(1,41):\n",
    "        pipe = Pipeline([('poly',PolynomialFeatures(j)),\n",
    "                             ('reg', LinearRegression(copy_X = True))])\n",
    "        pipe.fit(x.reshape(-1,1),y)\n",
    "        \n",
    "        pred = pipe.predict(x_test.reshape(-1,1))\n",
    "        test_errors[i,j-1] = np.sum((pred - y_test)**2)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.plot(range(1,41), np.mean(test_errors,axis=0))\n",
    "\n",
    "plt.xlabel(\"Polynomial Degree\", fontsize = 16)\n",
    "plt.ylabel(\"Mean Test Set Error\", fontsize = 16)\n",
    "\n",
    "plt.scatter(np.argmin(np.mean(test_errors,axis=0)) + 1, \n",
    "         np.min(np.mean(test_errors,axis=0)), s=40, c=\"black\")\n",
    "\n",
    "plt.text(np.argmin(np.mean(test_errors,axis=0)) - 1, \n",
    "         np.min(np.mean(test_errors,axis=0)-.2), \n",
    "         \"Lowest Error\", fontsize = 14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we suspected the lowest mean error occurred with the degree $2$ model. This is the model where Bias and Variance are equal or close to equal.\n",
    "\n",
    "\n",
    "This isn't the last time we'll see the bias-variance trade-off in this course. A general rule of thumb to remember is that the \"simpler\" the model the more likely it is to have high bias$^2$ (underfit), the more \"complex\" a model the more likely it is to have high variance (overfit). Where what \"simple\" and \"complex\" means depends on the context of the problem.\n",
    "\n",
    "That's it for this notebook. We'll learn about two techniques to have more complex models while avoiding overfitting in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2021.\n",
    "\n",
    "Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
