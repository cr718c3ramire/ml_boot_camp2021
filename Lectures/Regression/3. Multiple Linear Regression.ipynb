{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "So far we've covered simple linear regression. As we saw, this algorithm/model is quite limitted in what it can accomplish. We'll now expand this model to make a more versatile model.\n",
    "\n",
    "## What We'll Accomplish in This Notebook\n",
    "\n",
    "In this notebook we'll do the following:\n",
    "\n",
    "- Set up the multiple linear regression statistical model,\n",
    "- Derive the least squares estimate for the model,\n",
    "- Discuss modeling of categorical variables with one-hot encoding,\n",
    "- Introduce interaction terms,\n",
    "- Show how to model polynomial and other nonlinear terms in a linear regression setting\n",
    "\n",
    "Remember my note about the math from the last notebook, if you're not a math person don't become panicked if there is math content you don't entirely understand. You can always ask questions during a breakout session or on Slack :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages we'll use\n",
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# We'll use this later\n",
    "from numpy import meshgrid\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# This is new, it will allow us to interact with\n",
    "# 3d plots in the notebook\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Changes Going From Feature to Features?\n",
    "\n",
    "### The Statistical Model\n",
    "\n",
    "Assume that our data set has $n$ observations.\n",
    "\n",
    "Recall that in simple linear regression the statistical model is:\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 X + \\epsilon,\n",
    "$$\n",
    "where $X$ is an $n\\times 1$ feature vector, $y$ is an $n\\times 1$ target vector, and $\\epsilon$ is an $n\\times 1$ vector of independent $\\epsilon_i \\sim N(0,\\sigma^2)$ for all $i$. \n",
    "\n",
    "\n",
    "For multiple linear regression instead of a single feature we have $m$ features. In this setting the model becomes:\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_m X_m + \\epsilon ,\n",
    "$$\n",
    "where $X_1,X_2,\\dots,X_m$ are the $m$ features, and $\\epsilon$ is the same as in SLR. If we collect all of the features as a single column along with a column of $1$s in an $n \\times m$ feature matrix,\n",
    "$$\n",
    "X = \\left(\\begin{array}{c | c | c | c | c}\n",
    "    1 & X_1 & X_2 & \\dots & X_m\n",
    "\\end{array}\\right),\n",
    "$$\n",
    "and let $\\beta=\\left(\\beta_0, \\beta_1, \\beta_2,\\dots,\\beta_m\\right)^T$ then the statistical model for MLR becomes:\n",
    "$$\n",
    "y = X\\beta + \\epsilon.\n",
    "$$\n",
    "\n",
    "In the explanatory modeling setting we're still making all of the same assumptions as in SLR, but now $y$ is a linear function of $m$ predictors instead of just $1$. \n",
    "\n",
    "### Estimating the Model\n",
    "\n",
    "When it comes to estimating $\\beta$ we can get away with just using `sklearn` or `statsmodels`. However it's useful to know how to calculate the estimate 'by hand'.\n",
    "\n",
    "#### Minimizing MSE\n",
    "\n",
    "We again set out to minimize the MSE\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^n \\left( y_i - \\hat{y_i} \\right)^2 = \\frac{1}{n} \\sum_{i=1}^n \\left( y_i - X_{i,\\bullet} \\hat{\\beta} \\right)^2 = \\frac{1}{n} (y-X\\beta)^T ( y-X\\beta) = y^T y - \\beta^TX^Ty - y^T X \\beta + \\beta^T X^T X \\beta.\n",
    "$$\n",
    "\n",
    "Taking the derivative with respect to $\\beta$ and setting equal to $0$ gives:\n",
    "$$\n",
    "X^T X \\beta - X^T y = 0, \\text{ and so } \\hat{\\beta} = \\left( X^TX \\right)^{-1} X^T y.\n",
    "$$\n",
    "This is the estimate of $\\beta$ that minimizes the MSE, you may hear people refer to this as the normal equation or the Ordinary Least Squares (OLS) Solution.\n",
    "\n",
    "### Predicting Sales Based on Ad Buys\n",
    "\n",
    "Now that we understand the MLR setup. Let's use it to model sales given how much money was spent on various advertising mediums. The data is stored in `Advertising.csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in the data\n",
    "ads = pd.read_csv(\"Advertising.csv\")\n",
    "\n",
    "print(\"There are\", len(ads), \"observations in the data set.\")\n",
    "print(\"The columns are\", ads.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the train test split\n",
    "ads_copy = ads.copy()\n",
    "\n",
    "## Set aside 20% of the data\n",
    "## make 614 the random_state\n",
    "ads_train = ads_copy.sample(frac = .80, random_state = 614)\n",
    "ads_test = ads_copy.drop(ads_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine training head\n",
    "ads_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set came from the book, <a href=\"https://www.statlearning.com/\">Introduction to Statistical Learning</a>, by  Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. \n",
    "\n",
    "The data can be found here, <a href=\"https://www.statlearning.com/resources-first-edition\">https://www.statlearning.com/resources-first-edition</a>.\n",
    "\n",
    "How can we decide which features to include? We'll dive more into model selection in the next notebook, for now let's look at what features have the strongest correlation with `sales`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads_train.corr()['sales'].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like `radio` and `TV` are the most promising when it comes to having a linear relationship. Both have relatively strong positive linear relationships according to the Pearson correlation. In addition to examining $r$, this data set has so few features we can produce what is called a scatter matrix. <i>This should look familiar to those that did the python prep notebooks<i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scatter matrix makes a matrix of scatter plots\n",
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first input the dataframe you want to see a \n",
    "## scatter matrix for\n",
    "## then enter figsize and other plotting arguments\n",
    "scatter_matrix(ads_train, figsize = (12,12), alpha = 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There definitely appears to be some relationship between `sales` and `TV` as well as `sales` and `radio`. For now we'll treat these as linear relationships, but stay tuned.\n",
    "\n",
    "It seems that a reasonable starting model would be\n",
    "$$\n",
    "\\text{sales} = \\beta_0 + \\beta_1 \\text{TV} + \\beta_2 \\text{radio} + \\epsilon\n",
    "$$\n",
    "Let's fit the coefficients using the formula we just learned before just using `sklearn` or `statsmodels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make X\n",
    "X_train = np.ones([len(ads_train),3])\n",
    "X_train[:,1] = np.array(ads_train.TV)\n",
    "X_train[:,2] = np.array(ads_train.radio)\n",
    "\n",
    "## Make y\n",
    "y_train = np.array(ads_train.sales)\n",
    "\n",
    "## Calculate beta_hat\n",
    "beta_hat = np.linalg.inv(X_train.transpose().dot(X_train)).dot(X_train.transpose()).dot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"beta_0_hat is\",np.round(beta_hat[0],5))\n",
    "print(\"beta_1_hat is\",np.round(beta_hat[1],5))\n",
    "print(\"beta_2_hat is\",np.round(beta_hat[2],5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You Code\n",
    "\n",
    "Use `statsmodels` and `sklearn` to fit the model we just fit by hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the packages we'll need here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the model using statsmodels here\n",
    "## Be sure to print out the table\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the model using sklearn here\n",
    "## store your model object in reg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print out the coefficients and then intercept \n",
    "## from the sklearn model here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can use sklearn to make a prediction on the\n",
    "## training data as well, do so here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Note Before Continuing: A Slight Change in Interpretation\n",
    "\n",
    "We can still interpret the fit, but the specific details are slightly different. Let's interpret $\\hat{\\beta_1}$, this has been estimated as $0.04645$. We can interpret this as:\n",
    "\n",
    "<i>for a $1$ unit increase in TV holding all other variables constant, we estimate a $0.04645$ increase in sales.</i>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Regression\n",
    "\n",
    "This is more difficult with multiple features. Because we only have two features we can make a $3$-D plot like so.\n",
    "\n",
    "The code below will produce a mesh grid of input values for `TV` and `radio`. We'll then use that grid to generate the model predictions and plot them in a 3-D graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression(copy_X = True)\n",
    "\n",
    "reg.fit(ads_train[['TV','radio']],ads_train['sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the prediction this is where meshgrid comes in handy\n",
    "x1s = np.linspace(ads_train.TV.min(), ads_train.TV.max(), 10)\n",
    "x2s = np.linspace(ads_train.radio.min(), ads_train.radio.max(), 10)\n",
    "\n",
    "## this makes a meshgrid\n",
    "x1v, x2v = np.meshgrid(x1s, x2s)\n",
    "\n",
    "## combine x1v and x2v into a single array for prediction\n",
    "X_grid = np.concatenate([x1v.reshape(-1,1), x2v.reshape(-1,1)], axis=1)\n",
    "\n",
    "## predict at each grid point\n",
    "pred_grid = reg.predict(X_grid)\n",
    "\n",
    "## add the predictions to the grid array\n",
    "X_grid = np.concatenate([X_grid, pred_grid.reshape(-1,1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "## Now we plot the regression plane\n",
    "## along with the training observations\n",
    "\n",
    "## Make a figure object\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "\n",
    "## We'll add a 3d subplot object\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "## plot_trisurf makes a surface out of triangles\n",
    "## it will take in the TV grid on the x-axis\n",
    "## the radio grid on the y-axis\n",
    "## and the prediction grid on the z-axis\n",
    "## alpha <1 allows us to see through the surface\n",
    "ax.plot_trisurf(X_grid[:, 0], X_grid[:, 1], X_grid[:,2], alpha=0.4)\n",
    "\n",
    "## scatter will plot the observations from the training set\n",
    "ax.scatter(ads_train['TV'], ads_train['radio'], ads_train['sales'], c=\"r\", alpha=1, label=\"Training Data\")\n",
    "\n",
    "## Add labels\n",
    "ax.set_xlabel(\"TV\", fontsize=14)\n",
    "ax.set_ylabel(\"radio\", fontsize=14)\n",
    "ax.set_zlabel(\"sales\", fontsize=14)\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can rotate the plot and examine how well we fit the data with our model. As you rotate it does seem like the residuals may not be random, so perhaps our model was not the best choice. We'll examine this more closely in later in the notebook when we introduce transformations of features and interaction terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to do this because of\n",
    "# the %matplotlib notebook argument in the above code block\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You Code\n",
    "\n",
    "Return once again to the `carseats` data set. Examine the continuous features using correlations and scatter plots and produce a MLR model to predict `Sales`. Just use your best judgement, it's okay if you don't produce the best model right now. We will be touching on model selection later.\n",
    "\n",
    "Note you can read about the variables in `carseats` here, <a href=\"https://rdrr.io/cran/ISLR/man/Carseats.html\">https://rdrr.io/cran/ISLR/man/Carseats.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in the carseats data\n",
    "carseats = pd.read_csv(\"carseats.csv\")\n",
    "\n",
    "## Make the same train test split as the last notebook\n",
    "carseats_train = carseats.copy().sample(frac=.75,random_state = 440)\n",
    "carseats_test = carseats.copy().drop(carseats_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try plotting a scatter matrix here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You'll need to run this after\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit a Model here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here (if needed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Qualitative Predictors with Beer Data\n",
    "\n",
    "Now that we have a grasp of how MLR works with continuous predictors let's examine how we can also include qualitative predictors like binary or categorical features.\n",
    "\n",
    "To do this we'll look at `beer.csv`, our goal is to model `IBU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in the csv\n",
    "beers = pd.read_csv(\"beer.csv\")\n",
    "\n",
    "print(\"There are\", len(beers), \"observations in the data set.\")\n",
    "print(\"The columns are\", beers.columns)\n",
    "\n",
    "## Check the percentage of each Beer_Type to\n",
    "## see if we need to stratify our train test split\n",
    "beers.Beer_Type.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train test split\n",
    "beers_copy = beers.copy()\n",
    "\n",
    "beers_train = beers_copy.sample(frac=.75,random_state = 614)\n",
    "beers_test = beers_copy.drop(beers_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sanity check to see our percentages are close \n",
    "## to the same as the original\n",
    "beers_train.Beer_Type.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Add Qualitative Data to Regression\n",
    "\n",
    "Now why might we want to add in this qualitative data? Because it can add more context. Let's use `seaborn`'s `lmplot` to demonstrate with the `beer` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## First plot a normal regression of ibu on abv\n",
    "sns.lmplot(data = beers_train, x=\"ABV\", y=\"IBU\", \n",
    "           height=6, ci = False)\n",
    "\n",
    "plt.xlabel(\"ABV\", fontsize=16)\n",
    "plt.ylabel(\"IBU\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now plot a regression with \"hue\" listed as Beer_Type\n",
    "sns.lmplot(data = beers_train, x=\"ABV\", y=\"IBU\", \n",
    "           hue = \"Beer_Type\", height=2,  \n",
    "           ci = False)\n",
    "\n",
    "plt.xlabel(\"ABV\", fontsize=16)\n",
    "plt.ylabel(\"IBU\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the idea is that while there may exist a linear relationship between `ABV` and `IBU`, it is possible that the exact relationship is different based on the type of beer you're looking at.\n",
    "\n",
    "Making a `lmplot` is one way to see if there is an effect from a qualitative variable, but it isn't always feasible, especially with a lot of possible qualitative values. Another way to probe for an effect is with a box and whisker plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use seaborn to make this\n",
    "fig,ax = plt.subplots(figsize=(6,8))\n",
    "\n",
    "# boxplot takes in the data, the x-axis variable, the y-axis variable\n",
    "# and produces a box and whisker plot\n",
    "sns.boxplot(data = beers_train,x = \"Beer_Type\",y = \"IBU\",ax=ax)\n",
    "\n",
    "plt.xlabel(\"Beer Type\", fontsize = 16)\n",
    "plt.ylabel(\"IBU\", fontsize = 16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A box plot is a way of showing the distribution of a continuous variable. The bottom of each box shows the $25^\\text{th}$ percentile of the distribution, the middle line shows the median of the distribution, and the $75^\\text{th}$ percentile of the distribution. \n",
    "\n",
    "For the `beers` data our plot shows that the distribution of `IBU` is slightly different by `Beer_Type`. Giving more evidence that we may want to include `Beer_Type` as a qualitative feature in the model.\n",
    "\n",
    "\n",
    "### How to Include Qualitative Features - One-Hot Encoding\n",
    "\n",
    "We cannot just include the `Beer_Type` variable in the model like `reg.fit(beers_train[['ABV','Beer_Type']],beers_train['IBV'])` because our features must be numeric not strings. So we must change our strings into numbers. The way this is done in general is called <i>one-hot encoding</i>.\n",
    "\n",
    "#### How to One-Hot Encode\n",
    "\n",
    "In general suppose your qualitative variable, $Q$, has $k$ possible values. In our example $k = 2$. Then you create $k-1$ new indicator variables,\n",
    "$$\n",
    "1_{l,i} = \\left\\lbrace \\begin{array}{c c}\n",
    "    0 & \\text{if } Q_i \\neq l \\\\\n",
    "    1 & \\text{if } Q_i = l\n",
    "\\end{array} \\right.,\n",
    "$$\n",
    "for $l$ being any of the first $k-1$ $k$ possible values, and where $i$ denotes the $i^\\text{th}$ observation in the data set. By the process of elimination if each of the $k-1$ $1_l$ are $0$ then the value of $Q$ must be the $k^\\text{th}$ possible value.\n",
    "\n",
    "So for our `beers` data we would create a `Stout` indicator that is $1$ if the beer is a `Stout` and $0$ if not.\n",
    "\n",
    "<i>Note: We only need $k-1$ indicators because of the process of elimination. In our example we only need $1$ indicator because if an observation is not a stout we know it is an ipa, and vice versa. Pay careful attention to the number of indicators you use in your model, this is a common mistake new modelers make.</i>\n",
    "\n",
    "Let's see how to create this in `python`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can do it by hand\n",
    "## First set all beers as 0 for the variable Stout\n",
    "beers_train['Stout'] = 0\n",
    "\n",
    "## Then locate all the stouts and set Stout to 1\n",
    "beers_train.loc[beers_train.Beer_Type == \"Stout\",'Stout'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beers_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## By hand is easy for a binary option, but\n",
    "## tedious for variables with many categories\n",
    "## in that case pandas.get_dummies is useful\n",
    "## https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html, docs\n",
    "\n",
    "pd.get_dummies(beers_train['Beer_Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For us we'd only keep the `Stout` column\n",
    "beers_train['Stout'] = pd.get_dummies(beers_train['Beer_Type'])['Stout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beers_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to fit the following model:\n",
    "$$\n",
    "\\text{IBU} = \\beta_0 + \\beta_1 \\text{ABV} + \\beta_2 \\text{Stout} + \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll use sklearn\n",
    "reg = LinearRegression(copy_X = True)\n",
    "\n",
    "## fit the model\n",
    "reg.fit(beers_train[['ABV','Stout']],beers_train['IBU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"beta_0_hat is\",np.round(reg.intercept_,5))\n",
    "print(\"beta_1_hat is\",np.round(reg.coef_[0],5))\n",
    "print(\"beta_2_hat is\",np.round(reg.coef_[1],5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here we'll plot our regression line\n",
    "\n",
    "## We can plot the two lines by limiting our prediction input\n",
    "stout_values = np.ones((100,2))\n",
    "stout_values[:,0] = np.linspace(beers_train['ABV'].min(),beers_train['ABV'].max(),100)\n",
    "\n",
    "## make a stout prediction\n",
    "stout_pred = reg.predict(stout_values)\n",
    "\n",
    "## make ipa values\n",
    "ipa_values = np.zeros((100,2))\n",
    "ipa_values[:,0] = np.linspace(beers_train['ABV'].min(),beers_train['ABV'].max(),100)\n",
    "ipa_pred = reg.predict(ipa_values)\n",
    "\n",
    "## Let's plot\n",
    "plt.figure(figsize=(6,5))\n",
    "\n",
    "## plot stout values\n",
    "plt.scatter(beers_train.loc[beers_train.Stout == 1,'ABV'], \n",
    "               beers_train.loc[beers_train.Stout == 1,'IBU'],\n",
    "               c = 'blue', alpha = .8, label=\"Stout Training Data\")\n",
    "\n",
    "## plot the stout line\n",
    "plt.plot(stout_values[:,0], stout_pred, \"k--\", label=\"Stout Regression Line\")\n",
    "\n",
    "## plot ipa values\n",
    "plt.scatter(beers_train.loc[beers_train.Stout == 0,'ABV'], \n",
    "               beers_train.loc[beers_train.Stout == 0,'IBU'],\n",
    "               c = 'orange', alpha = .8, label=\"IPA Training Data\")\n",
    "\n",
    "## plot the ipa line\n",
    "plt.plot(ipa_values[:,0], ipa_pred, \"k\", label=\"IPA Regression Line\")\n",
    "\n",
    "plt.legend(fontsize = 14)\n",
    "\n",
    "plt.xlabel(\"ABV\",fontsize=16)\n",
    "plt.ylabel(\"IBU\",fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why isn't My Line the Same?\n",
    "\n",
    "Now notice, this is NOT the line we produced above with the the `sns.lmplot` command. That's because we have not included the interaction between `ABV` and `Stout`. \n",
    "\n",
    "#### A Better Explanation - Look at the Model\n",
    "\n",
    "Recall our model\n",
    "$$\n",
    "\\text{IBU} = \\beta_0 + \\beta_1 \\text{ABV} + \\beta_2 \\text{Stout} + \\epsilon.\n",
    "$$\n",
    "\n",
    "When Stout $=0$ the intercept for our line is $\\beta_0$. When Stout $=1$ the intercept becomes $\\beta_0 + \\beta_2$. But, notice in either case the slope of the line stays the same, $\\beta_1$. If you look back at the `seaborn` plot we produced, the lines do not have the same slope. So clearly our statistical model is missing something that the `seaborn` model has. We'll return to this after you practice some coding.\n",
    "\n",
    "\n",
    "### You Code\n",
    "\n",
    "Take your model for the `carseats` data from earlier. Examine the feature labeled `ShelveLoc`. Does it appear that this qualitatve variable has an impact on `Sales`? Add this feature to the model you made earlier in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Examine ShelveLoc here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remember to run this if you make a plot\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make indicator variables here for ShelveLoc here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit your model with indicators here\n",
    "\n",
    "## Find the training MSE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction Terms\n",
    "\n",
    "Let's return to our issue with the beer regression line.\n",
    "\n",
    "Remember the model we fit:\n",
    "$$\n",
    "\\text{IBU} = \\beta_0 + \\beta_1 \\text{ABV} + \\beta_2 \\text{Stout} + \\epsilon.\n",
    "$$\n",
    "\n",
    "We noted that this model only allows for different intercepts for Stouts and IPAs, but not slopes. \n",
    "\n",
    "\n",
    "However if we add in the term $\\beta_3 \\text{Stout} \\times \\text{ABV}$ like so,\n",
    "$$\n",
    "\\text{IBU} = \\beta_0 + \\beta_1 \\text{ABV} + \\beta_2 \\text{Stout} + \\beta_3 \\text{Stout} \\times \\text{ABV} + \\epsilon\n",
    "$$\n",
    "\n",
    "then when Stout $=0$ the slope of the line is $\\beta_1$ and when Stout $=1$ the slope of the line $\\beta_1 + \\beta_3$. The $\\text{Stout} \\times \\text{ABV}$ is the interaction term between Stout and ABV.\n",
    "\n",
    "<i>Note in the Regression Homework Sets you are asked to interpret the coefficient estimates for this model.</i>\n",
    "\n",
    "Let's make an interaction term and then fit this new model with `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make the interaction term\n",
    "beers_train['Stout_ABV'] = beers_train['Stout']*beers_train['ABV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You make and fit the model here\n",
    "## use sklearn and store the model in reg\n",
    "reg = LinearRegression(copy_X = True)\n",
    "\n",
    "reg.fit(beers_train[['ABV','Stout','Stout_ABV']],beers_train['IBU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"beta_0_hat is\",np.round(reg.intercept_,5))\n",
    "print(\"beta_1_hat is\",np.round(reg.coef_[0],5))\n",
    "print(\"beta_2_hat is\",np.round(reg.coef_[1],5))\n",
    "print(\"beta_3_hat is\",np.round(reg.coef_[2],5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Again this code will plot the lines, don't worry about it\n",
    "### for now\n",
    "\n",
    "## We can plot the two lines by limiting our prediction input\n",
    "stout_values = np.ones((100,3))\n",
    "\n",
    "## make the ABV values\n",
    "stout_values[:,0] = np.linspace(beers_train['ABV'].min(),beers_train['ABV'].max(),100)\n",
    "\n",
    "## now the interaction term\n",
    "stout_values[:,2] = stout_values[:,0]*stout_values[:,1]\n",
    "\n",
    "## make a stout prediction\n",
    "stout_pred = reg.predict(stout_values)\n",
    "\n",
    "## make ipa values\n",
    "ipa_values = np.zeros((100,3))\n",
    "ipa_values[:,0] = np.linspace(beers_train['ABV'].min(),beers_train['ABV'].max(),100)\n",
    "ipa_values[:,2] = ipa_values[:,0]*ipa_values[:,1]\n",
    "ipa_pred = reg.predict(ipa_values)\n",
    "\n",
    "\n",
    "## Let's plot\n",
    "plt.figure(figsize=(6,5))\n",
    "\n",
    "## plot stout values\n",
    "plt.scatter(beers_train.loc[beers_train.Stout == 1,'ABV'], \n",
    "               beers_train.loc[beers_train.Stout == 1,'IBU'],\n",
    "               c = 'blue', alpha = .8, label=\"Stout Training Data\")\n",
    "\n",
    "## plot the stout line\n",
    "plt.plot(stout_values[:,0], stout_pred, \"k--\", label=\"Stout Regression Line\")\n",
    "\n",
    "## plot ipa values\n",
    "plt.scatter(beers_train.loc[beers_train.Stout == 0,'ABV'], \n",
    "               beers_train.loc[beers_train.Stout == 0,'IBU'],\n",
    "               c = 'orange', alpha = .8, label=\"IPA Training Data\")\n",
    "\n",
    "## plot the ipa line\n",
    "plt.plot(ipa_values[:,0], ipa_pred, \"k\", label=\"IPA Regression Line\")\n",
    "\n",
    "plt.legend(fontsize = 14)\n",
    "\n",
    "plt.xlabel(\"ABV\",fontsize=16)\n",
    "plt.ylabel(\"IBU\",fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this looks more like the `seaborn.lmplot`!\n",
    "\n",
    "Adding interaction terms can also help with non-random residual plots as we'll see in a next example of this notebook.\n",
    "\n",
    "### You Code\n",
    "\n",
    "Load the following data set called `inter`. Build a regression model regressing $y$ on $x_1$ and $x_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter = pd.read_csv(\"inter.csv\")\n",
    "\n",
    "inter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explore the categorical variable x2 with\n",
    "## sns.lmplot here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this to close the plot\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the appropriate indicator and interaction terms here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the model here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n",
    "\n",
    "Now not every relationship in the world is a line or a plane. \n",
    "\n",
    "Let's look at a synthetic dataset and see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"poly.csv\")\n",
    "\n",
    "print(\"There are\",len(df),\"observations.\")\n",
    "print(\"The columns are\",df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the train test split\n",
    "df_copy = df.copy()\n",
    "\n",
    "df_train = df_copy.sample(frac=.75,random_state = 614)\n",
    "df_test = df_copy.drop(df_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(df_train, figsize=(8,8))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there certainly appears to be a relationship between $x_1$ and $y$ and $x_2$ and $y$. While the relationship between $x_2$ and $y$ might be linear, the relationship between $x_1$ and $y$ is definitely not linear.\n",
    "\n",
    "One way to address this is to make a polynomial transformation of $x_1$. For instance $x_1^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['x1_sq'] = df_train['x1']**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(df_train, figsize=(8,8))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between $y$ and $x_1^2$ seems somewhat linear, let's now include it in a model:\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\beta_3 x_2 + \\epsilon,\n",
    "$$\n",
    "and then we'll fit this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression(copy_X = True)\n",
    "\n",
    "reg.fit(df_train[['x1','x1_sq','x2']], df_train['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"beta_0_hat is\",np.round(reg.intercept_,5))\n",
    "print(\"beta_1_hat is\",np.round(reg.coef_[0],5))\n",
    "print(\"beta_2_hat is\",np.round(reg.coef_[1],5))\n",
    "print(\"beta_3_hat is\",np.round(reg.coef_[2],5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the residual plot\n",
    "pred = reg.predict(df_train[['x1','x1_sq','x2']])\n",
    "\n",
    "res = df_train['y'] - pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,4))\n",
    "\n",
    "plt.scatter(pred,res)\n",
    "\n",
    "plt.xlabel(\"Predicted Values\", fontsize=16)\n",
    "plt.ylabel(\"Residuals\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely not random!\n",
    "\n",
    "An obviously nonrandom residual plot indicates that there is some signal in the data not being captured by our model. One way to address this is to add an interaction term. Let's try adding in $x_1 x_2$, so our model becomes:\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\beta_3 x_2 + \\beta_4 x_1 x_2 + \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First add the interaction term to the df\n",
    "df_train['x1_x2'] = df_train['x1']*df_train['x2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression(copy_X = True)\n",
    "\n",
    "reg.fit(df_train[['x1','x1_sq','x2','x1_x2']], df_train['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"beta_0_hat is\",np.round(reg.intercept_,5))\n",
    "print(\"beta_1_hat is\",np.round(reg.coef_[0],5))\n",
    "print(\"beta_2_hat is\",np.round(reg.coef_[1],5))\n",
    "print(\"beta_3_hat is\",np.round(reg.coef_[2],5))\n",
    "print(\"beta_4_hat is\",np.round(reg.coef_[3],5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's re-examine the residual plot\n",
    "pred = reg.predict(df_train[['x1','x1_sq','x2','x1_x2']])\n",
    "\n",
    "res = df_train['y'] - pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "plt.scatter(pred, res)\n",
    "\n",
    "plt.xlabel(\"Predicted Values\", fontsize=16)\n",
    "plt.ylabel(\"Residuals\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So much better!\n",
    "\n",
    "#### Which is the Best Model?\n",
    "\n",
    "While this certainly seems like the best model, we'll discuss how we can compare the two models in a more robust way in a later notebook.\n",
    "\n",
    "#### Do I Need to Include it?\n",
    "\n",
    "If you notice the coefficient on $x_1$ is close to $0$ in the interaction term model. It may be tempting to remove this feature from the model especially if the true relationship was:\n",
    "$$\n",
    "y = 2 + x_1^2 - 10 x_2 + x_1 x_2.\n",
    "$$\n",
    "However, there is no way for you to know ahead of time what the true relationship is between the target and the features, if there was there'd be no need for regression. \n",
    "\n",
    "To further illustrate this point, imagine the true relationship was such that:\n",
    "$$\n",
    "y \\propto x_1^2,\n",
    "$$\n",
    "if we do not include $x_1$ in our model we are limiting ourselves to parabolas of the form\n",
    "$$\n",
    "\\beta_0 + \\beta_1 x_1^2,\n",
    "$$\n",
    "which leaves out a number of possible parabolas.\n",
    "\n",
    "It is important to remember that anytime you make a model that includes a polynomial transformation you need to include all of the lesser powers as well. So with $x_1^2$ as the highest power you'd need to include $x_1$, with $x_1^3$ as the highest power you'd need to include $x_1^2$ and $x_1$, and so on for $x_1^n$.\n",
    "\n",
    "This also holds for interaction terms. If you include $x_1 x_2$ you need to include both $x_1$ and $x_2$ as predictors as well.\n",
    "\n",
    "\n",
    "### You Code\n",
    "\n",
    "The data labeled `df` below came from a job interview problem set. In the homework you'll have to build the best predictive model you can on the data. For now examine the relationship between `y` and `x1`. Build a model to predict `y` using `x1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data\n",
    "df = pd.read_csv(\"PredictiveModelingAssessmentData.csv\")\n",
    "\n",
    "## Make a train test split\n",
    "df_train = df.copy().sample(frac = .75, random_state = 440)\n",
    "df_test = df.copy().drop(df_train.index)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Examine the scatter matrix here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this to close the plot\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the appropriately transformed column\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the Regression here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Popular Transformations\n",
    "\n",
    "We can add in more than just polynomials. There are other popular transformations including $\\log$s, roots, $\\sin$, $\\cos$, $\\tan$, exponentials, and more.\n",
    "\n",
    "We'll work through an example returning to our `Advertising` data then you'll make a model for the interview data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(ads_train, figsize = (8,8), alpha = 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "\n",
    "plt.scatter(np.sqrt(ads_train.TV),ads_train.sales)\n",
    "\n",
    "plt.xlabel(\"$\\sqrt{TV}$\", fontsize = 16)\n",
    "plt.ylabel(\"sales\", fontsize = 16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much more linear. Let's replace `TV` in our model from Notebook 3 with root `TV`.\n",
    "$$\n",
    "\\text{sales} = \\beta_0 + \\beta_1 \\sqrt{\\text{TV}} + \\beta_2 \\text{radio} + \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in root tv to the df\n",
    "ads_train['sqrt_TV'] = np.sqrt(ads_train.TV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the model object\n",
    "reg = LinearRegression(copy_X = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data\n",
    "reg.fit(ads_train[['sqrt_TV','radio']],ads_train['sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"beta_0_hat is\",np.round(reg.intercept_,5))\n",
    "print(\"beta_1_hat is\",np.round(reg.coef_[0],5))\n",
    "print(\"beta_2_hat is\",np.round(reg.coef_[1],5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the residual plot\n",
    "pred = reg.predict(ads_train[['sqrt_TV','radio']])\n",
    "\n",
    "res = ads_train['sales'] - pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "plt.scatter(pred,res)\n",
    "\n",
    "plt.xlabel(\"Predicted Values\", fontsize=16)\n",
    "plt.ylabel(\"Residuals\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again not random. Let's add in the interaction term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads_train['sqrtTV_radio'] = ads_train['sqrt_TV'] * ads_train['radio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the model object\n",
    "reg = LinearRegression(copy_X = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the data\n",
    "reg.fit(ads_train[['sqrt_TV','radio','sqrtTV_radio']],ads_train['sales'])\n",
    "\n",
    "## We'll want to look at these later\n",
    "print(reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's re-examine the residual plot\n",
    "pred = reg.predict(ads_train[['sqrt_TV','radio','sqrtTV_radio']])\n",
    "\n",
    "res = ads_train['sales'] - pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,4))\n",
    "\n",
    "plt.scatter(pred,res)\n",
    "\n",
    "plt.xlabel(\"Predicted Values\", fontsize=16)\n",
    "plt.ylabel(\"Residuals\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You Code\n",
    "\n",
    "Return to the `PredictiveModelingAssessmentData.csv` data set. Does it appear that $y$ and $x_2$ have a linear relationship?\n",
    "\n",
    "Try playing around with different non-linear transformations of $x_2$ (note don't use polynomials)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for this notebook in the next notebook we'll discuss model selection for predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2021.\n",
    "\n",
    "Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
