{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLR Predictive Model Selection\n",
    "\n",
    "So now we know how to build a wide array of linear regression models. For instance if our data set contained $n$ observations of $m$ features we could build $2^m$ models without even considering interactions, polynomials, or other nonlinear transformations of the data. That's a lot of models to choose from, so in this notebook we'll introduce how you might go about selecting the best multiple linear regression model.\n",
    "\n",
    "\n",
    "## What We'll Accomplish in This Notebook\n",
    "\n",
    "In this notebook we'll:\n",
    "- Have a discussion of generalization error in predictive models\n",
    "- Introduce the concept of cross-validation\n",
    "- Use best subset selection for the Advertising data set\n",
    "- Practice building the best predictive model, using the tools we've learned so far\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages we'll use\n",
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# We'll use this later\n",
    "from numpy import meshgrid\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization Error\n",
    "\n",
    "As we've discussed before we can't just choose the model that performs the best on our training data because we could just arbitrarily make a model that fits each training point to its target value.\n",
    "\n",
    "We may also be tempted to see which model performs best on the test set we made, but this has a similar problem. We'll just be producing models that perform really well on the test set.\n",
    "\n",
    "We want to get a sense of how well our model will perform on any new random data pull, this is known as the <i>generalization error</i>.\n",
    "\n",
    "What can we do to get a sense for the generalization error of our model?\n",
    "\n",
    "### Cross-Validation\n",
    "\n",
    "Enter $k$-fold cross-validation.\n",
    "\n",
    "The idea behind this technique is pretty clever. We'll introduce the technique in a way that's generalizes beyond linear regression, which will come in handy down the line.\n",
    "\n",
    "When building a predictive model, the model estimate (what we've been calling $\\hat{f}$) is found from minimizing a <i>loss function</i>. For linear regression this loss function was the MSE of the training data. Let's consider the case where we will randomly draw a new set of data (think test set) and see how well our estimate performs. Because the data was randomly drawn, the algorithm we use is a random process. So the value of the loss function (the generalization error) on this new data is an example of a random variable, let's call this random variable $G$.\n",
    "\n",
    "It would be nice to know something about the distribution of $G$, but this is tricky with a finite amount of data. However, we can leverage a popular theorem from probability theory called the law of large numbers (see the probability theory and statistics cheat sheet).\n",
    "\n",
    "If we were able to generate a bunch of random draws of $G$, say $k$ random draws, then the law of large numbers says that:\n",
    "$$\n",
    "\\frac{1}{k}\\sum_{i=1}^k G_i \\approx E(G),\n",
    "$$\n",
    "assuming our random draws were independent.\n",
    "\n",
    "So in $k$-fold cross-validation we take our training set, and randomly split it into $k$ equally sized chunks. For each chunk we train the algortihm on the $k-1$ other chunks and then calculate the testing loss using the chunk we left out. Then we take the arithmetic mean of all $k$ testing errors. This is an approximation of the expected value of the true generalization error of the algorithm.\n",
    "\n",
    "Here's a picture to help illustrate the idea for $5$-fold cross-validation:\n",
    "<img src=\"CV_pic.png\" style=\"width:80%\"></img>\n",
    "\n",
    "Let's see how this works in `sklearn` to help us choose the best model for the `Advertising` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads = pd.read_csv(\"Advertising.csv\")\n",
    "ads_copy = ads.copy()\n",
    "\n",
    "ads_train = ads_copy.sample(frac=.75, random_state=614)\n",
    "ads_test = ads_copy.drop(ads_train.index)\n",
    "\n",
    "## remember in notebook 3 we found these\n",
    "## to be useful\n",
    "ads_train['sqrt_TV'] = np.sqrt(ads_train['TV'])\n",
    "ads_train['sqrtTV_radio'] = ads_train['sqrt_TV']*ads_train['radio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import the KFold object from sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "## We'll need this when we fit models\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we make a kfold object\n",
    "## we'll use 5 splits\n",
    "## and shuffle the data before making the splits\n",
    "kfold = KFold(n_splits = 5, shuffle = True, random_state = 440)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [  3   4   5   6   7   8   9  10  11  12  13  15  17  18  19  20  21  22\n",
      "  23  24  25  26  27  28  29  31  33  34  35  36  37  38  39  40  41  42\n",
      "  43  44  45  46  47  49  50  51  53  55  56  57  60  61  62  63  64  65\n",
      "  67  68  69  70  71  72  73  74  75  77  79  81  82  83  84  85  86  87\n",
      "  88  89  91  92  93  95  97 100 101 102 105 107 108 110 111 112 113 114\n",
      " 115 116 117 118 119 120 121 122 123 124 125 128 129 130 131 133 134 135\n",
      " 136 137 138 140 141 142 143 144 145 146 147 149] TEST: [  0   1   2  14  16  30  32  48  52  54  58  59  66  76  78  80  90  94\n",
      "  96  98  99 103 104 106 109 126 127 132 139 148]\n",
      "TRAIN: [  0   1   2   4   5   6   8   9  10  11  12  13  14  15  16  18  21  22\n",
      "  24  25  26  27  28  29  30  32  34  35  37  39  40  42  43  44  45  46\n",
      "  47  48  49  50  51  52  53  54  55  56  58  59  60  61  62  64  65  66\n",
      "  67  68  69  70  73  75  76  77  78  80  81  82  83  84  85  86  87  88\n",
      "  90  93  94  95  96  97  98  99 101 102 103 104 105 106 109 110 111 112\n",
      " 113 114 115 117 118 119 121 122 124 125 126 127 129 130 131 132 133 134\n",
      " 135 136 138 139 140 141 142 144 146 147 148 149] TEST: [  3   7  17  19  20  23  31  33  36  38  41  57  63  71  72  74  79  89\n",
      "  91  92 100 107 108 116 120 123 128 137 143 145]\n",
      "TRAIN: [  0   1   2   3   7   9  11  12  14  15  16  17  18  19  20  21  22  23\n",
      "  27  28  29  30  31  32  33  34  35  36  38  39  41  42  43  45  47  48\n",
      "  49  50  52  54  55  56  57  58  59  61  62  63  66  67  68  69  70  71\n",
      "  72  73  74  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  94  95  96  97  98  99 100 103 104 106 107 108 109 110 111 112\n",
      " 114 115 116 118 120 121 122 123 126 127 128 129 131 132 133 134 135 136\n",
      " 137 139 140 141 142 143 144 145 146 147 148 149] TEST: [  4   5   6   8  10  13  24  25  26  37  40  44  46  51  53  60  64  65\n",
      "  75  93 101 102 105 113 117 119 124 125 130 138]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8  10  13  14  16  17  18  19  20  21\n",
      "  22  23  24  25  26  28  29  30  31  32  33  34  35  36  37  38  40  41\n",
      "  43  44  46  48  49  50  51  52  53  54  56  57  58  59  60  61  62  63\n",
      "  64  65  66  67  71  72  74  75  76  77  78  79  80  81  82  83  84  89\n",
      "  90  91  92  93  94  95  96  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109 110 111 113 114 116 117 118 119 120 122 123 124 125 126 127 128 130\n",
      " 132 134 135 136 137 138 139 143 145 146 148 149] TEST: [  9  11  12  15  27  39  42  45  47  55  68  69  70  73  85  86  87  88\n",
      "  97 112 115 121 129 131 133 140 141 142 144 147]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  19  20  23  24  25  26  27  30  31  32  33  36  37  38  39  40  41  42\n",
      "  44  45  46  47  48  51  52  53  54  55  57  58  59  60  63  64  65  66\n",
      "  68  69  70  71  72  73  74  75  76  78  79  80  85  86  87  88  89  90\n",
      "  91  92  93  94  96  97  98  99 100 101 102 103 104 105 106 107 108 109\n",
      " 112 113 115 116 117 119 120 121 123 124 125 126 127 128 129 130 131 132\n",
      " 133 137 138 139 140 141 142 143 144 145 147 148] TEST: [ 18  21  22  28  29  34  35  43  49  50  56  61  62  67  77  81  82  83\n",
      "  84  95 110 111 114 118 122 134 135 136 146 149]\n"
     ]
    }
   ],
   "source": [
    "## To make this simpler I make my data\n",
    "## into an array\n",
    "X = np.array(ads_train[['TV','radio']])\n",
    "y = np.array(ads_train['sales'])\n",
    "\n",
    "# You can loop through all the splits like so\n",
    "for train_index, test_index in kfold.split(X,y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now let's put it all together.\n",
    "## It will be easier for us to make a couple functions\n",
    "## to loop through all 8 possible models\n",
    "\n",
    "## This gets our data for us\n",
    "def get_X_y(df,features,target):\n",
    "    # Returns X then y\n",
    "    return np.array(df[features]), np.array(df[target])\n",
    "\n",
    "## this calculates the mse\n",
    "def get_mse(model, X, y):\n",
    "    # get the prediction\n",
    "    pred = model.predict(X)\n",
    "    \n",
    "    # Returns the mse\n",
    "    return np.sum(np.power(pred-y,2))/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['TV'],\n",
       " ['radio'],\n",
       " ['TV', 'radio'],\n",
       " ['newspaper'],\n",
       " ['TV', 'newspaper'],\n",
       " ['radio', 'newspaper'],\n",
       " ['TV', 'radio', 'newspaper'],\n",
       " ['sqrt_TV'],\n",
       " ['TV', 'sqrt_TV'],\n",
       " ['radio', 'sqrt_TV'],\n",
       " ['TV', 'radio', 'sqrt_TV'],\n",
       " ['newspaper', 'sqrt_TV'],\n",
       " ['TV', 'newspaper', 'sqrt_TV'],\n",
       " ['radio', 'newspaper', 'sqrt_TV'],\n",
       " ['TV', 'radio', 'newspaper', 'sqrt_TV'],\n",
       " ['sqrtTV_radio'],\n",
       " ['TV', 'sqrtTV_radio'],\n",
       " ['radio', 'sqrtTV_radio'],\n",
       " ['TV', 'radio', 'sqrtTV_radio'],\n",
       " ['newspaper', 'sqrtTV_radio'],\n",
       " ['TV', 'newspaper', 'sqrtTV_radio'],\n",
       " ['radio', 'newspaper', 'sqrtTV_radio'],\n",
       " ['TV', 'radio', 'newspaper', 'sqrtTV_radio'],\n",
       " ['sqrt_TV', 'sqrtTV_radio'],\n",
       " ['TV', 'sqrt_TV', 'sqrtTV_radio'],\n",
       " ['radio', 'sqrt_TV', 'sqrtTV_radio'],\n",
       " ['TV', 'radio', 'sqrt_TV', 'sqrtTV_radio'],\n",
       " ['newspaper', 'sqrt_TV', 'sqrtTV_radio'],\n",
       " ['TV', 'newspaper', 'sqrt_TV', 'sqrtTV_radio'],\n",
       " ['radio', 'newspaper', 'sqrt_TV', 'sqrtTV_radio'],\n",
       " ['TV', 'radio', 'newspaper', 'sqrt_TV', 'sqrtTV_radio']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function was modified from stackexchange user hughdbrown \n",
    "# at this link, \n",
    "# https://stackoverflow.com/questions/1482308/how-to-get-all-subsets-of-a-set-powerset\n",
    "\n",
    "# This returns the power set of a set minus the empty set\n",
    "def powerset_no_empty(s):\n",
    "    power_set = []\n",
    "    x = len(s)\n",
    "    for i in range(1 << x):\n",
    "        power_set.append([s[j] for j in range(x) if (i & (1 << j))])\n",
    "            \n",
    "    return power_set[1:]\n",
    "\n",
    "powerset_no_empty(['TV','radio','newspaper','sqrt_TV','sqrtTV_radio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_features = powerset_no_empty(['TV','radio','newspaper','sqrt_TV','sqrtTV_radio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now make an array that will hold the mses\n",
    "## for all the models\n",
    "## the columns represent each possible model\n",
    "MSEs = np.empty((5,len(possible_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a regression model\n",
    "reg = LinearRegression(copy_X = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## keep track of what split we're on\n",
    "i = 0\n",
    "\n",
    "## This is for the initial input into the kfold object\n",
    "X, y = get_X_y(ads_train, possible_features[0], 'sales')\n",
    "\n",
    "## Perform CV\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    ## For each possible model\n",
    "    for j in range(len(possible_features)):\n",
    "        ## get X and y\n",
    "        X, y = get_X_y(ads_train, possible_features[j], 'sales')\n",
    "\n",
    "\n",
    "        # Get the cv train test split\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Cloning the regression makes a fresh regression \n",
    "        # model for each run\n",
    "        clone_reg = clone(reg)\n",
    "        \n",
    "        # fit the model\n",
    "        clone_reg.fit(X_train,y_train)\n",
    "        \n",
    "        MSEs[i,j] = get_mse(clone_reg, X_test, y_test)\n",
    "    \n",
    "    ## We'll now move to the next split\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.45349694, 25.89255369,  3.42614468, 38.06769672, 11.40099804,\n",
       "        26.06179983,  3.46317853, 10.12814808,  9.99206445,  2.30472053,\n",
       "         2.36439288, 11.12514329, 10.95156229,  2.32605452,  2.38254855,\n",
       "         4.51766001,  0.54986328,  1.91184979,  0.6330742 ,  4.48432301,\n",
       "         0.54839638,  1.90260051,  0.6315345 ,  0.19498507,  0.20727558,\n",
       "         0.33284623,  0.3263197 ,  0.19411623,  0.20634827,  0.33290576,\n",
       "         0.32640856],\n",
       "       [ 9.71572054, 17.44793455,  2.04434287, 26.42582728,  7.5302255 ,\n",
       "        17.45154021,  2.05566141, 11.04508259, 10.59977115,  1.70909788,\n",
       "         1.84441754,  8.58721354,  8.16905575,  1.7742603 ,  1.94300778,\n",
       "         3.19231167,  0.52882723,  0.60539351,  0.30572488,  3.27949869,\n",
       "         0.52863329,  0.60054779,  0.3061925 ,  0.28325437,  0.27958298,\n",
       "         0.17599377,  0.17106165,  0.29045898,  0.29077882,  0.17605366,\n",
       "         0.17232471],\n",
       "       [12.21872735, 16.02296025,  4.3957273 , 24.27057911, 10.47995636,\n",
       "        16.09505964,  4.47561529, 11.10699638, 11.68921855,  2.37106871,\n",
       "         2.38945743,  9.66956005,  9.93043696,  2.39574296,  2.42000776,\n",
       "         3.92342786,  1.00693407,  0.63201263,  0.48710634,  3.87298878,\n",
       "         1.03192673,  0.63215535,  0.49463297,  0.48231958,  0.45416174,\n",
       "         0.30172342,  0.28608775,  0.48917308,  0.4607882 ,  0.30430673,\n",
       "         0.2879843 ],\n",
       "       [ 9.56393218, 15.10069217,  1.73921495, 28.13847369,  9.36276767,\n",
       "        15.9896412 ,  1.7485408 ,  9.7332701 ,  9.51114837,  1.74380546,\n",
       "         1.96757318,  9.56110078,  9.32535397,  1.74419632,  1.96758202,\n",
       "         3.05777495,  0.34412595,  0.80980513,  0.26868694,  2.99027432,\n",
       "         0.34328507,  0.8514071 ,  0.28056333,  0.23558323,  0.25895814,\n",
       "         0.15260258,  0.15560391,  0.235342  ,  0.25857501,  0.15868939,\n",
       "         0.16103641],\n",
       "       [13.74114649, 14.16516924,  2.28195035, 18.337289  , 14.08671111,\n",
       "        13.8456363 ,  2.50241955, 13.19474925, 13.47128528,  1.35346093,\n",
       "         1.28158778, 13.24474427, 13.69625134,  1.46259596,  1.38459069,\n",
       "         3.7924586 ,  0.22318571,  0.72486228,  0.09729847,  3.91185913,\n",
       "         0.23433793,  0.71656778,  0.10649388,  0.08433199,  0.07874122,\n",
       "         0.06431287,  0.07235932,  0.08895942,  0.08055993,  0.07037735,\n",
       "         0.07588959]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here are the MSEs\n",
    "MSEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.1386047 , 17.72586198,  2.77747603, 27.04797316, 10.57213174,\n",
       "       17.88873544,  2.84908311, 11.04164928, 11.05269756,  1.8964307 ,\n",
       "        1.96948576, 10.43755239, 10.41453206,  1.94057001,  2.01954736,\n",
       "        3.69672662,  0.53058725,  0.93678467,  0.35837817,  3.70778879,\n",
       "        0.53731588,  0.94065571,  0.36388344,  0.25609485,  0.25574393,\n",
       "        0.20549577,  0.20228647,  0.25960994,  0.25941004,  0.20846658,\n",
       "        0.20472871])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We can get the mean MSE for each model \n",
    "## across the CV splits like so\n",
    "np.mean(MSEs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We can get the mean MSEs using np.mean\n",
    "## axis = 0 tells np.mean to take the column mean\n",
    "## we can get where the min occurs with argmin\n",
    "np.argmin(np.mean(MSEs, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model with the lowest mean CV MSE was the one with ['TV', 'radio', 'sqrt_TV', 'sqrtTV_radio'] as the features. This model had a mean CV MSE of 0.20229\n"
     ]
    }
   ],
   "source": [
    "print(\"The model with the lowest mean CV MSE\",\n",
    "     \"was the one with\", possible_features[np.argmin(np.mean(MSEs, axis = 0))],\n",
    "     \"as the features. This model had a mean CV MSE of\",\n",
    "     np.round(np.min(np.mean(MSEs, axis=0)),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44976"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Another popular measure is root mse\n",
    "## this is because it has the same dimension as \n",
    "## the target variable\n",
    "\n",
    "## It can be interpreted as how far off we were from the\n",
    "## true value on average\n",
    "np.round(np.min(np.sqrt(np.mean(MSEs, axis = 0))),5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we brute forced our way through all the models because we had a small number of predictors and a decent sample size. \n",
    "\n",
    "This really doesn't work when your data has a large number of predictors (there are too many models to check, and you need a large quantity of samples to fit models with a lot of predictors this is known as the curse of dimensionality), or if you have a small sample size (makes it difficult to split your data even further). We'll learn other techniques for model selection in those cases later on. Also in many cases it doesn't make sense to even include a predictor in the model, for example because it has no association with your target.\n",
    "\n",
    "\n",
    "\n",
    "There are two other algorithms we touch on in the HW called backwards and forwards selection that are greedy algorithms. The benefit of these approaches is that they run more quickly than brute force, the problem is that they might not give you the \"best model\".\n",
    "\n",
    "\n",
    "For now we'll stick to examining correlations, scatter plots to provide plausible features, and then use cross validation to pick the best subset of those plausible features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You Code\n",
    "\n",
    "Go ahead and try to build the best model you can to predict `carseat` `sales`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "carseats = pd.read_csv(\"carseats.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "No sample solution for this problem since participants may come up with a wide array of models.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2021.\n",
    "\n",
    "Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the ErdÅ‘s Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
