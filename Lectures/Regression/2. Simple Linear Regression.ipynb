{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression (SLR)\n",
    "\n",
    "## What We'll Accomplish in This Notebook\n",
    "\n",
    "In this notebook we'll introduce regression tasks with the most basic regression model, simple linear regression (slr). Specifically we will:\n",
    "- Discuss what a regression task is,\n",
    "- Introduce the stastical model behind simple linear regression,\n",
    "- Learn about train test splits,\n",
    "- Look at explanatory slr, and\n",
    "- End with predictive slr.\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Note on Math\n",
    "\n",
    "This will be our first notebook with a fair amount of math. While the specific math we cover today isn't the most advanced thing we'll cover, if at any point you find yourself lost in the math of any of the boot camp's notebooks that's okay! Don't feel like you need to understand everything the first time you see it in lecture. Also, it is possible to understand how to apply various data science algorithms and techniques without fully understanding the math working behind the scenes.\n",
    "\n",
    "Finally, if at any point you have a question feel free to ask me or any of our breakout room leads. If you don't feel comfortable asking during lecture or a breakout session you can always message me in Slack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Modeling\n",
    "\n",
    "Recall our set up from the previous notebook.\n",
    "\n",
    "We've collected some features, $X$, and some output data, $y$, and we believe there is some true relationship between the two:\n",
    "$$\n",
    "y = f(X) + \\epsilon,\n",
    "$$\n",
    "called the <i>model</i>.\n",
    "\n",
    "Regression tasks are the name that we'll give problems in which the output data is a numeric type. It is common to say that we are regressing $y$ on $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "We'll first learn the algorithm and then address concerns specific to explanatory and predictive settings. Let's use an example to introduce the concept.\n",
    "\n",
    "You work for ESPN and you think that you can predict a baseball team's win total if you know their run differential. \n",
    "\n",
    "<i>For non-baseball fans a run is considered $1$ point in baseball, and run differential is the number of runs your team scores, denoted as $r$, minus the number of runs your team gave up, denoted as $ra$ for runs allowed, so run differential is $rd = r - ra$.</i>\n",
    "\n",
    "This is a regression problem, we want to predict a quantitative outcome, wins, using some feature, run differential.\n",
    "\n",
    "Let's look at some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to import the data\n",
    "# it is stored in the baseball_run_diff.csv file\n",
    "baseball = pd.read_csv(\"baseball_run_diff.csv\")\n",
    "\n",
    "\n",
    "# This will tell us about the df\n",
    "print(\"There are\",len(baseball),\"observations in the baseball df.\")\n",
    "print(\"The columns are\",list(baseball.columns))\n",
    "\n",
    "# Look at 5 randomly sampled rows\n",
    "baseball.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Test Split\n",
    "\n",
    "You may be tempted to start exploring this data. However, since we'll eventually be building a predictive model on top of it we want to set aside a small subset of the data aside for testing purposes. This is known as making the <i>train test split</i>.\n",
    "\n",
    "The <i>train set</i> is the one we build our model on. A machine learning algorithm uses the train set to estimate the true relationship the best it can. \n",
    "\n",
    "Good performance on the training data does not guarantee good predictive performance. So in order to get a sense of how good a particular model is we set aside a <i>test set</i> at the beginning of the model building process. This subset of the total data set is meant to allow you to test your model on data it didn't train on, therefore allowing you to simulate predicting on entirely new data. Typically this data isn't touched until the end of the model building process.\n",
    "\n",
    "For a number of reasons the training data can lead us to believe we have a better model than we actually have. The test data exists as a sanity check to see if the model is as good as the training data tells us.\n",
    "\n",
    "##### Using `pandas` for a train test split\n",
    "\n",
    "So let's use `pandas` to make our train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first make a copy of the original dataframe\n",
    "## this is due to the way python internally \n",
    "## stores objects\n",
    "baseball_copy = baseball.copy()\n",
    "\n",
    "## Now use sample to make a random sample\n",
    "## frac allows us to choose a fraction of the df\n",
    "## it is common to set aside 25% for testing\n",
    "baseball_train = baseball_copy.sample(frac = .75, random_state = 440)\n",
    "\n",
    "## now use drop and the train index to make test\n",
    "baseball_test = baseball_copy.drop(baseball_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now look at the training set head\n",
    "baseball_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can explore!\n",
    "\n",
    "Let's examine any potential relationship between wins (coded as `W`) and run differential (coded as `RD`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll use plt.scatter for this\n",
    "\n",
    "## first make a figure\n",
    "## this makes a figure that is 8 units by 8 units\n",
    "plt.figure(figsize = (8,8))\n",
    "\n",
    "## plt.scatter plots RD on the x and W on the y\n",
    "plt.scatter(baseball_train.RD, baseball_train.W)\n",
    "\n",
    "## Always good practice to label well when\n",
    "## presenting a figure to others\n",
    "## place an xlabel\n",
    "plt.xlabel(\"Run Differential\", fontsize =16)\n",
    "\n",
    "## place a ylabel\n",
    "plt.ylabel(\"Wins\", fontsize = 16)\n",
    "\n",
    "## type this to show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do you see here?\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### The SLR Model\n",
    "\n",
    "Let $y$ denote our output variable, $X$ denote our features (in this case feature), and $\\epsilon \\sim N(0,\\sigma^2)$. Then our statistical model for slr is:\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 X + \\epsilon \n",
    "$$\n",
    "for some constants $\\beta_0$ and $\\beta_1$.\n",
    "\n",
    "Thus for the $i^{\\text{th}}$ observation we say that:\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i,\n",
    "$$\n",
    "where $\\epsilon_i$ is a draw from $\\epsilon$'s distribution. Importantly we also assume that the observations are independent of one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the Model - What is \"Best\"\n",
    "\n",
    "While the idea of \"best\" is slightly different between predictive and explanatory modeling, fitting the model is identical in both cases. Let $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ denote our estimates from the data. We want to choose $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ so that the $\\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1} X_i$ are collectively as close to the $y_i$ as possible. A common way to measure collective closeness is by considering the sum of the square errors, in particular we'll use the <i>Mean Square Error (MSE)</i> or equivalently the <i>Root Mean Square Error</i> (=$\\sqrt{\\text{MSE}}$). \n",
    "\n",
    "Suppose our estimates are $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$, then the MSE for the model using those estimates is\n",
    "$$\n",
    "MSE = \\frac{1}{n}\\sum_{i=1}^n(\\hat{y_i} - y_i)^2 = \\frac{1}{n}\\sum_{i=1}^n(\\hat{\\beta_0} + \\hat{\\beta_1}X_i - y_i)^2.\n",
    "$$\n",
    "The \"best\" estimates will be the $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ that minimize the MSE. Doing some Calculus you can derive that this gives the following best linear estimates for the $\\hat{\\beta}$s:\n",
    "$$\n",
    "\\hat{\\beta_1} = \\frac{\\sum_{i=1}^n\\left(X_i - \\overline{X}\\right)\\left(y_i - \\overline{y}\\right)}{\\sum_{i=1}^n\\left(X_i - \\overline{X}\\right)^2} = \\frac{\\text{cov}(X,y)}{\\sigma^2_{X}}, \\text{ and}\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "\\hat{\\beta_0} = \\overline{y} - \\hat{\\beta_1}\\overline{X}.\n",
    "$$\n",
    "These two formulas give the <i>least squares coefficient estimates</i> for simple linear regression.\n",
    "\n",
    "Let's do a little coding and calculate the least squares estimate for regressing wins on run differential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note you can get a column's variance using .var()\n",
    "## Note you can get a covariance matrix from a dataframe using .cov()\n",
    "\n",
    "## We get the means here\n",
    "X_bar = baseball_train.RD.mean()\n",
    "y_bar = baseball_train.W.mean()\n",
    "\n",
    "\n",
    "## We get the covariance and variance\n",
    "cov = baseball_train[['RD','W']].cov().iloc[0,1]\n",
    "var = baseball_train.RD.var()\n",
    "\n",
    "## Here we calculate beta_1_hat\n",
    "beta_1_hat = cov/var\n",
    "\n",
    "## Here we calculate beta_0_hat\n",
    "beta_0_hat = y_bar - beta_1_hat*X_bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can produce a plot with the fitted line here\n",
    "\n",
    "## Use the x as the input for your line, i.e. beta_0_hat + beta_1_hat*x\n",
    "min_rd = baseball_train.RD.min()\n",
    "max_rd = baseball_train.RD.max()\n",
    "padding = 20\n",
    "x = np.linspace(min_rd - padding,max_rd + padding,1000)\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "# plt.scatter plots RD on the x and W on the y\n",
    "plt.scatter(baseball_train.RD, baseball_train.W, label = \"observations\")\n",
    "\n",
    "# Now add mean prediction line\n",
    "plt.plot(np.linspace(min_rd - padding,max_rd + padding,1000),\n",
    "            beta_0_hat + beta_1_hat*np.linspace(min_rd - padding,max_rd + padding,1000), 'k',\n",
    "            label=\"SLR Line\",\n",
    "            linewidth = 3)\n",
    "\n",
    "# Always good practice to label well when\n",
    "# presenting a figure to others\n",
    "# place an xlabel\n",
    "plt.xlabel(\"Run Differential\", fontsize =16)\n",
    "\n",
    "# place a ylabel\n",
    "plt.ylabel(\"Wins\", fontsize = 16)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "# type this to show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break for Questions\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn\n",
    "\n",
    "Now it's your turn to practice building a SLR model.\n",
    "\n",
    "We'll look at the `carseats` data set stored as `carseats.csv`. The data comes from <a href=\"https://www.statlearning.com/\">Introduction to Statistical Learning in R</a>. and looks at various variables related to the sales of child car seats at $400$ stores. Each row is a store. The thing we'd like to predict is `Sales`. We'll return to this data set in later notebooks. For now try to build a SLR model regressing `Sales` on `Price`. Make a plot that includes your regression line and the training data.\n",
    "\n",
    "Remember to just get as far as you can in the time alotted, you can always come back to the notebook after lecture is over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code Here - \n",
    "## Read in the Data and make a train test split with pandas\n",
    "## Look at the head of the dataframe\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code Here\n",
    "\n",
    "## Make a scatter plot examining the relationship between\n",
    "## Sales and Price here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Here\n",
    "\n",
    "## Calculate beta_1_hat and beta_0_hat\n",
    "## for the model regressing Sales on Price\n",
    "\n",
    "## Get the means here\n",
    "X_bar_cars = \n",
    "y_bar_cars = \n",
    "\n",
    "\n",
    "## Get the covariance and variance\n",
    "cov_cars = \n",
    "var_cars = \n",
    "\n",
    "## Calculate beta_1_hat\n",
    "beta_1_hat_cars = \n",
    "\n",
    "## Calculate beta_0_hat\n",
    "beta_0_hat_cars = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Here\n",
    "\n",
    "## plot your regression line on top of the training data here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLR - Explanatory Modeling\n",
    "\n",
    "Let's now examine simple linear regression from the perspective of explanatory modeling.\n",
    "\n",
    "A first step is to discuss the theoretical assumptions we make on the data and how to check that they are satisfied. In explanatory modeling this is an important step because certain assumptions allow us to prove statistical properties about the least squares estimates that are desirable, and allow us to later derive uncertainty bounds on our estimates.\n",
    "\n",
    "### Assumption 1 - A Linear Relationship\n",
    "\n",
    "There exists a linear relationship between $y$ and $X$. This one is pretty straightforward. If we are going to model $y$ as a linear function of $X$ then we better think that one actually exists.\n",
    "\n",
    "#### How to check\n",
    "\n",
    "##### Make a plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use plt.scatter for this\n",
    "\n",
    "# first make a figure\n",
    "# this makes a figure that is 10 units by 10 units\n",
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "# plt.scatter plots RD on the x and W on the y\n",
    "plt.scatter(baseball_train.RD, baseball_train.W)\n",
    "\n",
    "# Always good practice to label well when\n",
    "# presenting a figure to others\n",
    "# place an xlabel\n",
    "plt.xlabel(\"Run Differential\", fontsize =16)\n",
    "\n",
    "# place a ylabel\n",
    "plt.ylabel(\"Wins\", fontsize = 16)\n",
    "\n",
    "# type this to show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or\n",
    "\n",
    "##### Look at the Pearson Correlation, $\\rho$\n",
    "\n",
    "This is a statistical measure of the <i>strength of the linear relationship</i> between $y$ and $X$. Here's the formula:\n",
    "$$\n",
    "\\rho = \\frac{Cov(y,X)}{\\sigma_X \\sigma_y}\n",
    "$$\n",
    "\n",
    "You can approximate $\\rho$ with the sample covariance and sample standard deviations, the approximation is $r$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We calculate the Pearson correlation for the baseball data here\n",
    "\n",
    "r = cov/(np.sqrt(baseball_train.RD.var())*np.sqrt(baseball_train.W.var()))\n",
    "\n",
    "\n",
    "\n",
    "print(\"The pearson coef is\",np.round(r,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the Pearson Correlation is bounded between $-1$ and $1$ with more positive values indicating a strong positive linear relationship, and more negative values indicating a strong negative linear relationship. Our correlation of $0.933$ here indicates that wins and run differential have a strong positive linear relationship.\n",
    "\n",
    "For those looking for more statistical rigor, check out this page, <a href=\"https://online.stat.psu.edu/stat501/lesson/1/1.9\">https://online.stat.psu.edu/stat501/lesson/1/1.9</a> to see how you can actually perform a hypothesis test on whether or not there is statistically significant evidence that $\\rho \\neq 0$.\n",
    "\n",
    "Note that getting a $\\rho$ close to $0$ does NOT mean there is no relationship, just no linear relationship. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find the correlation coefficient of the following data\n",
    "# Docs: https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html\n",
    "\n",
    "x = np.linspace(-2,2,1000)\n",
    "\n",
    "# y = x^2\n",
    "y = np.power(x,2)\n",
    "\n",
    "np.corrcoef(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumption 2 - All Observations are Independent\n",
    "\n",
    "We also assume that each observation in the data set is independent from all other observations.\n",
    "\n",
    "#### How to Check\n",
    "\n",
    "This is slightly harder to check then assessing a linear fit. But there are some ways to test.\n",
    "\n",
    "##### Thinking About How the Data Was Collected\n",
    "\n",
    "This approach helps if you know something about the data collection process. For example, if you wanted to know something about OSU undergrads and you randomly sampled people from a list in the registrars office that would produce independent observations. But if you randomly sampled people from a single Calculus Class and a single Art History Class then your observations are likely dependent.\n",
    "\n",
    "In our baseball example, we do have reason for concern. The data is produced from year after year observations of the same teams. We did sample our data randomly, but its possible that there is time dependence or team dependence. Which takes us to the next way to check.\n",
    "\n",
    "##### Making more plots\n",
    "\n",
    "Plot your residuals (this is what we call the difference between the predicted values and the actual values, $y_i - \\hat{y_i}$) against your feature and other variables of concern. Then you examine the plots to see if there is an obvious relationship. Let's do that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Residuals are actual values minus estimated values\n",
    "res =  baseball_train.W.values - (beta_0_hat + beta_1_hat * baseball_train.RD.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First plot of rd vs residual\n",
    "\n",
    "plt.figure(figsize = (12,12))\n",
    "\n",
    "plt.scatter(baseball_train.RD,res)\n",
    "\n",
    "plt.xlabel(\"Run Differential\", fontsize = 16)\n",
    "plt.ylabel(\"Residual\", fontsize = 16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plot of run differential vs residual it seems that the two have no obvious relationship, which is a good sign for our independence assumption. Let's move on to residual vs year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot year vs residual\n",
    "\n",
    "plt.figure(figsize = (12,12))\n",
    "\n",
    "plt.scatter(baseball_train.yearID,res)\n",
    "\n",
    "plt.xlabel(\"Year\", fontsize = 16)\n",
    "plt.ylabel(\"Residual\", fontsize = 16)\n",
    "\n",
    "# Note this allows you to set specific tick mark values\n",
    "plt.xticks(range(2000,2020))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another blob with no obvious pattern, excellent! This is more good news for us in terms of the independence assumption. Let's now look at residuals vs team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot team vs residual\n",
    "\n",
    "plt.figure(figsize = (16,12))\n",
    "\n",
    "plt.scatter(baseball_train.teamID,res)\n",
    "\n",
    "plt.xlabel(\"Team\", fontsize = 16)\n",
    "plt.ylabel(\"Residual\", fontsize = 16)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is a little worrying. Some teams seem to have their wins consistently overestimated (a negative residual) like ARIzona, while others are more likely to be underestimated (a positive residual) like MINnesota. So there is reason to believe that residuals are not independent over team. There are ways to handle this, but they fall out of the scope of this notebook. For this data our violation doesn't seem too egregious so we'll soldier on. \n",
    "\n",
    "### Assumption 3 - The Residuals are Normally Distributed with Mean 0 and Equal Variance\n",
    "\n",
    "The final assumption is on the distribution of the residuals. We say that $\\epsilon_i \\sim N(0,\\sigma^2)$ for all $i$. The assumption on variance is called the homoscedasticity assumption.\n",
    "\n",
    "#### How to Check Normality\n",
    "\n",
    "##### Make More Plots!\n",
    "\n",
    "We can make a histogram of the residuals to check for a bell curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram of the residuals here\n",
    "\n",
    "plt.hist(res)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This package allows us to make a q-q plot\n",
    "import statsmodels.api as sm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "# qqplot makes the qqplot\n",
    "# put in the data you want to plot\n",
    "# line = 's' plots a line fit to our data\n",
    "# ax=ax allows us to put the data on the plt subplot object I made\n",
    "sm.qqplot(res,line='s',ax=ax) \n",
    "\n",
    "# Normals go on the x-axis\n",
    "plt.xlabel(\"Normal Quantiles\", fontsize=16)\n",
    "\n",
    "# Residuals on the y-axis\n",
    "plt.ylabel(\"Residual Quantiles\", fontsize=16)\n",
    "\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good! There's typically some slight bowing of the dots near the tails of the normal distribution. We should feel okay with our assumption that the baseball residuals are normally distributed.\n",
    "\n",
    "#### How to Check Homoscedasticity?\n",
    "\n",
    "##### Even More Plots!\n",
    "\n",
    "Now we plot the predicted values vs the residuals. If the residuals have equal variance you should expect to see most of the points fall in a band around $0$. We DON'T want to see the points opening up into a funnel shape or closing into a funnel shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the predicted values on the x-axis\n",
    "# and the residuals on the y-axis\n",
    "\n",
    "pred = beta_0_hat + beta_1_hat * baseball_train.RD.values\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "\n",
    "plt.scatter(pred,res)\n",
    "\n",
    "plt.xlabel(\"Predicted Values\", fontsize = 16)\n",
    "plt.ylabel(\"Residuals\", fontsize = 16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again this looks good to me! The residuals mostly fall between $10$ and $-10$ and I don't see anything that could be construed as a funnel! Looks like we have homoscedasticity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break for Questions\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You Code\n",
    "\n",
    "Return to the model you made for the `carseats` data. Go through and check all of the linear regression assumptions.\n",
    "\n",
    "As always, it's okay if you don't finish in the alotted time, just get as far as you can :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code Here\n",
    "\n",
    "## Check the linearity assumption here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code Here\n",
    "\n",
    "## Check for independence Here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Here\n",
    "\n",
    "## Check the normality assumption here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code Here\n",
    "\n",
    "## Check the homoscedasticity here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $p$-value For $\\beta_1$\n",
    "\n",
    "So far we've fit a model, interpreted the fit, and examined whether or not our modeling assumptions fit.\n",
    "\n",
    "For the baseball data we're fairly confident that our assumptions are okay. So the next thing we can do from an explanatory modeling point of view is to assess the model fit. \n",
    "\n",
    "We can conduct a hypothesis test of whether or not there actually is a linear relationship between $y$ and $X$. In the view of SLR this means performing the following hypothesis test:\n",
    "$$\n",
    "\\text{H}_0: \\beta_1 = 0 \\text{ vs. }\n",
    "$$\n",
    "$$\n",
    "\\text{H}_1: \\beta_1 \\neq 0\n",
    "$$\n",
    "One way we can perform this test is to use the `statsmodel` package.\n",
    "\n",
    "We'll fit the regression in `statsmodel` then examine the model's summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a simple linear regression model\n",
    "# sm.OLS stands for Ordinary Least Squares, this\n",
    "# is the name for the method used to get the coefficients\n",
    "# First put the output then the features\n",
    "fit = sm.OLS(baseball_train['W'],sm.add_constant(baseball_train['RD']),).fit()\n",
    "\n",
    "# fit.summary makes a snazy table for us to look at\n",
    "print(fit.summary())\n",
    "print(beta_1_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's examine the table.\n",
    "<img src=\"SLR_table_p_value.png\" style=\"width:80%;\"></img>\n",
    "The row with `RD` corresponds to $\\beta_1$. We can see in the `coef` column the same estimate we calculated, $0.0999$. If you move over to the `P>|t|` column, this is the $p$-value for the hypothesis test we are interested in, for this particular data our $p$-value is smaller than $0.001$ which means that the probability of observing what we observed under the null hypothesis is very very small. This means we would reject the null hypothesis in favor of H$_1$. In non-statistical lingo, we have good reason to believe that there is a linear relationship between run differential and wins.\n",
    "\n",
    "### Confidence Interval for $\\beta_1$\n",
    "\n",
    "We can also construct a $95\\%$ confidence interval for $\\beta_1$. Recall from our Probability Theory and Statistics Cheat Sheet that this can be found by taking $\\hat{\\beta_1} \\pm p_{\\hat{\\beta_1},.95} se(\\hat{\\beta_1})$, under the assumptions of SLR the probability multiplier, $p_{\\hat{\\beta_1},.95}$ follows a studentized $t$ distribution with $n - 2$ degrees of freedom.\n",
    "\n",
    "In the homework you will work through a derivation of this confidence interval.\n",
    "\n",
    "We can go through the trouble of calculating this by hand, or we can rely on our prior `statsmodel` table\n",
    "<img src=\"SLR_table_CI.png\" style=\"width:80%;\"></img>\n",
    "So for the $\\beta_1$ from the baseball data our $95\\%$ confidence interval is $(0.096,0.104)$.\n",
    "\n",
    "\n",
    "### You Code\n",
    "\n",
    "Go through and perform the hypothesis test for $\\beta_1$ from the `carseats` data. Also construct a $95\\%$ confidence interval for $\\beta_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Coeficient of SLR (Read Through During a Breakout)\n",
    "\n",
    "A nice thing about SLR is that we can interpret the $\\beta_1$ coefficient in a meaningful way. If for example $\\hat{\\beta_1} = 2$ in the run differential problem we could say that for a $1$ point increase in run differential we estimate an increase of $2$ wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A print statement that interprets the \n",
    "# best linear estimate of beta_1 here\n",
    "print(\"A 1 point increase in run differential gives an estimated\",\n",
    "         np.round(beta_1_hat,4),\n",
    "          \"additional wins.\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"So we estimate that a team needs a\",\n",
    "         np.round(1/beta_1_hat,1),\n",
    "          \"point run differential increase for 1 additional win.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You Code\n",
    "\n",
    "Interpret the coefficient you got from the `carseats` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLR - Predictive Modeling\n",
    "\n",
    "The primary difference between explanatory and predictive modeling in the realm of slr is that for predictive modeling we do not necessarily care about the statistical assumptions we just finished discussing. In this setting we only care about making good predictions. That's not to say that those assumptions are not valuable. As a prime example the assumption of a linear relationship is probably an important feature of a good predictive SLR model.\n",
    "\n",
    "To end the notebook we'll introduce how to build a slr model using `sklearn`. `sklearn` is <b>the</b> open source python machine learning library. As we go through the course we'll be relying heavily on `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Building with sklearn\n",
    "\n",
    "# first we import Linear Regression from sklearn\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we make a model object\n",
    "slr = LinearRegression(copy_X = True)\n",
    "\n",
    "## Now we fit the model\n",
    "## first goes the input variables\n",
    "## Then the output variables\n",
    "## If the input is a 1-D vector you need to reshape it\n",
    "slr.fit(baseball_train['RD'].values.reshape(-1,1), baseball_train['W'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we can see the coefficients\n",
    "print(\"beta_1_hat is\", slr.coef_[0])\n",
    "print(\"beta_0_hat is\", slr.intercept_)\n",
    "\n",
    "print()\n",
    "\n",
    "## Let's compare that to what we computed\n",
    "print(\"We computed beta_1_hat to be\", beta_1_hat)\n",
    "print(\"We compute beta_0_hat to be\", beta_0_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can make predictions like so\n",
    "min_rd = baseball_train.RD.min()\n",
    "max_rd = baseball_train.RD.max()\n",
    "padding = 20\n",
    "x = np.linspace(min_rd - padding,max_rd + padding,10)\n",
    "\n",
    "slr.predict(x.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pattern we just showed is the common `sklearn` pattern, import the model, make a model object, fit the object, then predict. To learn more about the `LinearRegression` object read the documentation here: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Testing Performance\n",
    "\n",
    "Before setting you loose to work on a Simple Linear Regression problem of your own we can see how to measure the MSE of our model on both the training data and test data.\n",
    "\n",
    "##### Training Data\n",
    "\n",
    "It is often useful to compare how good the model fit training data when we are trying to choose a best model. Since there is no other model to choose from, we'll just get some practice coding in python below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function calculates the MSE\n",
    "def mse(y,y_pred,r):\n",
    "    return np.round(np.sum((y-y_pred)**2)/len(y),r)\n",
    "\n",
    "\n",
    "## Here we calculate the MSE on the training data\n",
    "y_train = baseball_train.W.values\n",
    "y_train_pred = slr.predict(baseball_train.RD.values.reshape(-1,1))\n",
    "\n",
    "print(\"The training MSE is\",mse(y_train,y_train_pred,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing Data\n",
    "\n",
    "Since we won't be doing any additional model improvements or model comparisons go ahead and calculate how well we did on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we can get it for the testing data\n",
    "y_test = baseball_test.W.values\n",
    "y_test_pred = slr.predict(baseball_test.RD.values.reshape(-1,1))\n",
    "\n",
    "\n",
    "print(\"The training MSE is\",mse(y_test,y_test_pred,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You Code\n",
    "\n",
    "Now it's your turn to practice building an SLR model `sklearn`.\n",
    "\n",
    "Build your `carseats` model using `sklearn`. What are the estimates of $\\beta_0$ and $\\beta_1$? Use it to find the training and test MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code Here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "## Calculate the training MSE here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "## Calculate the test MSE here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's it!\n",
    "\n",
    "That's it for simple linear regression. This may have been pretty straight forward, but it is our first algorithm! Next week we'll introduce multiple linear regression, which will allow us to build better models!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2021.\n",
    "\n",
    "Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
