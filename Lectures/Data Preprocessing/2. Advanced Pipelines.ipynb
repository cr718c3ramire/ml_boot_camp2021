{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Pipelines\n",
    "\n",
    "We now have a pretty strong repetoire of regression models. Depending on the data set there may be a number of preprocessing steps that should be taken prior to fitting the model. While we've learned basic pipelines and out of the box transformer objects you may need to perform preprocessing tasks that are too complicated for these simple tools.\n",
    "\n",
    "## What We'll Accomplish in This Notebook\n",
    "\n",
    "- We'll review the differences and necessity for fit, transform and fit_transform\n",
    "- Introduce the popular California Housing Data Set\n",
    "- Demonstrate how to construct custom transformer objects for more advanced pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import packages\n",
    "\n",
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `fit`, `transform`, and `fit_transform`\n",
    "\n",
    "Hopefully you remember from the `Basic Pipelines` notebook the terms, `fit`, `transform` and `fit_transform`. Let's return to the `StandardScaler` object as a reminder.\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the documentation listed above we know that the standard scaler will take in the features, `X`, and scale them like so:\n",
    "$$\n",
    "\\frac{X_i - \\overline{X_i}}{s_{X_i}}.\n",
    "$$\n",
    "\n",
    "Let's generate some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 10*np.random.randn(100,1)-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The mean of X is\",np.mean(X))\n",
    "print(\"The variance of X is\",np.var(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll scale $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first we make a scaler object\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Then we fit it\n",
    "scaler.fit(X)\n",
    "\n",
    "print(\"The scaler was fit to have mean\",scaler.mean_)\n",
    "print(\"and variance\",scaler.var_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The we transform the data, aka scale it\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The mean of X is\",np.mean(X_scaled))\n",
    "print(\"The standard deviation of X is\",np.std(X_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's imagine we're ready to check the test error on our model. So we have to scale the test features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = 10*np.random.randn(100,1)-5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The mean of X_test is\",np.mean(X_test))\n",
    "print(\"The variance of X_test is\",np.var(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what code should we write to scale the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(np.mean(X_test_scaled))\n",
    "\n",
    "print(np.var(X_test_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order in which these sorts of steps gets done is important. \n",
    "\n",
    "This is because you only fit the model on the training data, and the scaler (and other preprocessing steps) is thought of as part of the model. \n",
    "\n",
    "Let's do a short practice\n",
    "\n",
    "### You Code\n",
    "\n",
    "#### A New Scaler\n",
    "\n",
    "Go to the documentation and read about the `MinMaxScaler` object, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html</a>.\n",
    "\n",
    "Use the `MinMaxScaler` to scale the following training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your train and test data\n",
    "X_train = np.random.randint(1,1000,1000)\n",
    "X_test = np.random.randint(1,1000,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import MinMaxScaler here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit and transform the training and test data \n",
    "## using a MinMaxScaler here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing Values\n",
    "\n",
    "Sometimes your data may have missing values. It is often bad practice to throw away missing values, one option is to instead <i>impute</i> them.\n",
    "\n",
    "Imputation is when you use the non-missing values to fill in the missing values. Three simple ways would be to replace the missing values with the mean, median, or mode of the training data.\n",
    "\n",
    "Here is the documentation on the `SimpleImputer`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\">https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html</a>.\n",
    "\n",
    "We'll now impute the missing values on the following data using the median of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here is some data\n",
    "X_train = np.random.randn(1000)\n",
    "X_test = np.random.randn(1000)\n",
    "\n",
    "## With some values missing\n",
    "X_train[np.random.choice(range(1000),20)] = np.nan\n",
    "X_test[np.random.choice(range(1000),20)] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the SimpleImputer\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the imputer object with the desired \"strategy\"\n",
    "imp = SimpleImputer(strategy = 'median')\n",
    "\n",
    "print(\"X_train has\", sum(np.isnan(X_train)), \"missing values.\")\n",
    "\n",
    "## impute the missing values\n",
    "\n",
    "# first fit the imputer\n",
    "imp.fit(X_train.reshape(-1,1))\n",
    "\n",
    "# then transform\n",
    "X_train_imp = imp.transform(X_train.reshape(-1,1))\n",
    "\n",
    "print(\"After imputing X_train has\", sum(np.isnan(X_train_imp)), \"missing values.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now impute on the test data\n",
    "## note that we don't use the \"fit\" step here\n",
    "print(\"X_test has\", sum(np.isnan(X_test)), \"missing values.\")\n",
    "\n",
    "X_test_imp = imp.transform(X_test.reshape(-1,1))\n",
    "\n",
    "print(\"After imputing X_test has\", sum(np.isnan(X_test_imp)), \"missing values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The California Housing Data Set\n",
    "\n",
    "We'll now introduce a popular machine learning data set, the California Housing data set. The data is used in the book <a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\">Hands-On Machine Learning with Scikit-Learn and TensorFlow</a> as an example of a machine learning workflow. This is an excellent book and a useful reference if you're looking to purchase a book about machine learning with python.\n",
    "\n",
    "We won't be using this data to build a predictive model, but rather to demonstrate the need for advanced pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the data\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.copy().sample(frac=.75, random_state = 440)\n",
    "df_test = df.copy().drop(df_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's look at the dataframe info\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What kind of categories are possible for ocean proximity?\n",
    "df_train.ocean_proximity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each dot is at it's longitude and latitude\n",
    "## the size of the dot is proportional to its population\n",
    "## the color of the dot represents the median_house_value of the dot\n",
    "df_train.plot(kind=\"scatter\", x = \"longitude\", y = \"latitude\",\n",
    "             alpha = .9, s = df_train[\"population\"]/50, label=\"population\",\n",
    "             figsize=(12,14), c=\"median_house_value\",cmap = plt.get_cmap(\"viridis\"), \n",
    "             colorbar=True)\n",
    "\n",
    "plt.xlabel(\"Longitude\", fontsize=16)\n",
    "plt.ylabel(\"Latitude\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,14))\n",
    "sns.scatterplot(data=df_train,x=\"longitude\",y=\"latitude\",hue=\"ocean_proximity\")\n",
    "\n",
    "plt.xlabel(\"Longitude\", fontsize=16)\n",
    "plt.ylabel(\"Latitude\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now from our exploration of the data we can see that this data set has a number of preprocessing steps:\n",
    "1. `total_bedrooms` has a number of missing values that could be imputed\n",
    "2. `ocean_proximity` needs to be one-hot-encoded\n",
    "3. Many columns have vastly differing scales, so we should scale them\n",
    "4. We may want to create additional features from our other features.\n",
    "\n",
    "Now we'll review how to do 1. and 2. then it will be your job to incorporate 3. and 4. \n",
    "\n",
    "As we go through let's remember two main points:\n",
    "- Fitting should only be performed on the training set\n",
    "- A good pipeline takes in the features and target without any preprocessing and outputs the fit or prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing `total_bedrooms`\n",
    "\n",
    "Recall that we only want to impute the column for `total_bedrooms`. If we were to put `SimpleImputer` as is into the pipeline we'd be imputing the entire dataframe. While this isn't an issue for this dataset (because only `total_bedrooms` is missing data), it's an excellent time to introduce how you can create a custom imputer object.\n",
    "\n",
    "`sklearn` is quite nice because it gives us the functionality to make custom transformers relatively easily. To do this we make our own transformer object. \n",
    "\n",
    "To fully grasp everything going on check out the bonus content notebook in the `python prep` folder where I review objects and classes in python. If you're happy just copying and pasting the code for your own transformers (no shame in that for now, we're learning a lot of data science) no need to read through those notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll need these\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A python object is an instance of a python class.\n",
    "\n",
    "Below we define our `BedroomImputer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define our custom imputer\n",
    "class BedroomImputer(BaseEstimator, TransformerMixin):\n",
    "    # Class Constructor \n",
    "    # This allows you to initiate the class when you call\n",
    "    # BedroomImputer\n",
    "    def __init__(self):\n",
    "        # I want to initiate each object with\n",
    "        # the SimpleImputer method\n",
    "        self.SimpleImputer = SimpleImputer(strategy = \"median\")\n",
    "    \n",
    "    # For my fit method I'm just going to \"steal\"\n",
    "    # SimpleImputer's fit method using only the\n",
    "    # 'total_bedrooms' column\n",
    "    def fit(self, X, y = None ):\n",
    "        self.SimpleImputer.fit(X['total_bedrooms'].values.reshape(-1,1))\n",
    "        return self\n",
    "    \n",
    "    # Now I want to transform the total_bedrooms columns\n",
    "    # and return it with imputed values\n",
    "    def transform(self, X, y = None):\n",
    "        copy_X = X.copy()\n",
    "        copy_X['total_bedrooms'] = self.SimpleImputer.transform(copy_X['total_bedrooms'].values.reshape(-1,1))\n",
    "        return copy_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a custom imputer let's put it to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = BedroomImputer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.total_bedrooms.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit(df_train)\n",
    "\n",
    "imputer.transform(df_train).total_bedrooms.describe()\n",
    "\n",
    "imputer.fit_transform(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot-Encoding `ocean_proximity`\n",
    "\n",
    "Now let's see how we can one-hot-encode `ocean_proximity`.\n",
    "\n",
    "Here we can use the `FunctionTransformer` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our preprocessing function\n",
    "# This creates bedrooms_per_room\n",
    "# and one hot encodes ocean_proximity\n",
    "def one_hot_encode(df):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    hot_encoding = pd.get_dummies(df_copy['ocean_proximity'])\n",
    "    df_copy[hot_encoding.columns[:-1]] = hot_encoding[hot_encoding.columns[:-1]]\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = FunctionTransformer(one_hot_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot.transform(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!\n",
    "\n",
    "Now it's your turn.\n",
    "\n",
    "### You Code\n",
    "\n",
    "Your boss has told you that her end goal is to regress `median_house_value` on `median_income`, `ocean_proximity`, and a new feature, `bedrooms_per_room`.\n",
    "\n",
    "Write a function called `get_feats` that takes in a feature dataframe and returns the columns for `median_income` the one-hot-encoded `ocean_proximity`, and `bedrooms_per_room`. Feel free to use the function, `one_hot_encode` or not. Then create a `FunctionTransformer` object using `get_feats`, check to make sure that running `df_train` through your transformer object returns a dataframe with the desired columns, i.e. `median_income`, the one-hot-encoded `ocean_proximity` and `bedrooms_per_room`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you remember that it's important to scale the data prior to fitting your model. However, you only want to scale the columns for `median_income` and `bedrooms_per_room`, not the one-hot-encoded columns. Following the approach we took for `BedroomImputer` define a custom scaler called, `NumericScale` that takes in the dataframe produced by get_feats and scales the `median_income` and `bedrooms_per_room` columns. Hint: use `StandardScaler` in a manner similar to how `SimpleImputer` was used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Below is a SAMPLE SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put it all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's it!\n",
    "\n",
    "That's it for this notebook! You're now able to create more advanced pipelines which will make your code cleaner and more understandable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2021.\n",
    "\n",
    "Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
