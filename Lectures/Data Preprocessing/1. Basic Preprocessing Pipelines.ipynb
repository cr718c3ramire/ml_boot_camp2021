{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Preprocessing Pipelines\n",
    "\n",
    "When we have a number of preprocessing steps that we'd like to accomplish in a row prior to running an algorithm or fitting a model we use what are known as pipelines.\n",
    "\n",
    "## What We'll Accomplish in This Notebook\n",
    "\n",
    "- We'll introduce the concept of a pipeline\n",
    "- Show you how to build basic pipelines for polynomial and other nonlinear transforms\n",
    "- Demonstrate useful `sklearn` features for data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages we'll use\n",
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# We'll use this later\n",
    "from numpy import meshgrid\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Features\n",
    "\n",
    "Let's create a data set where we know for a fact that we'd like to perform polynomial regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(440)\n",
    "\n",
    "x = 10*np.random.random(1000) - 5\n",
    "\n",
    "y = 1 + x - 2*x**2 + .5*x**3 + 10*np.random.randn(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.scatter(x,y)\n",
    "\n",
    "plt.xlabel(\"$x$\", fontsize=16)\n",
    "plt.ylabel(\"y\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could create $x^2$ and $x^3$ features by hand, but a quicker and more easily reproducable way is to use the `PolynomialFeatures` `Transformer` object from `sklearn`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This creates a polynomial features object\n",
    "## with degree 3\n",
    "## and no bias column\n",
    "poly = PolynomialFeatures(degree=3, include_bias = False)\n",
    "\n",
    "## What is fit_transform? For now don't worry about it\n",
    "## We'll discuss it in greater depth later in the notebook\n",
    "print(\"\\t[x,\\t \\t x^2, \\t \\t x^3]\")\n",
    "poly.fit_transform(x.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For our own edification let's see what happens when\n",
    "## include_bias = True\n",
    "poly = PolynomialFeatures(degree=3, include_bias = True)\n",
    "\n",
    "print(\"\\t[x,\\t \\t x^2, \\t \\t x^3]\")\n",
    "poly.fit_transform(x.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's return to the correct polynomial transformer now\n",
    "poly = PolynomialFeatures(degree=3, include_bias = False)\n",
    "\n",
    "## Why just fit? Again let's be patient\n",
    "## we'll have the answer soon!\n",
    "poly.fit(x.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this correctly transformed data (no bias column) to fit a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can use this to fit a linear regression model\n",
    "slr = LinearRegression(copy_X = True)\n",
    "\n",
    "slr.fit(poly.transform(x.reshape(-1,1)),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = 1 + x - 2*x**2 + .5*x**3 + 10*np.random.randn(1000)\n",
    "print(slr.intercept_,slr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You Code\n",
    "\n",
    "Create the data below. Then try using `PolynomialFeatures` with the argument `interaction_only = True` to fit a regression model. Still use `include_bias=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here is the data you'll need to transform\n",
    "## Note everyone's final output will be slightly different\n",
    "## because there was no random seed\n",
    "x = np.random.randn(1000,2)\n",
    "\n",
    "## Here is the output you'll want to model\n",
    "y = x[:,0] + x[:,1] + 3*x[:,0]*x[:,1] + .5*np.random.randn(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the polynomial transformer here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit the regression model here\n",
    "## print out the regression intercept and coefficients and\n",
    "## compare them to the actual relationship used to generate y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Transformer\n",
    "\n",
    "`PolynomialFeatures` is great if we want the polynomial powers of all the features of the input array. However, as we've seen this is not always desirable. This is where `FunctionTransformer` comes into play. `FunctionTransformer` applies custom written functions to your data much like `PolynomialFeatures` applies the polynomial transformations. Let's use the `Advertising` data we looked at in `Regression` as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data\n",
    "ads = pd.read_csv(\"Advertising.csv\")\n",
    "ads_copy = ads.copy()\n",
    "\n",
    "ads_train = ads_copy.sample(frac = .75, random_state = 614)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the columns we want\n",
    "X_train = np.array(ads_train[['TV','radio']])\n",
    "y_train = np.array(ads_train['sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a function that returns an array with the squareroot of the \n",
    "## TV column\n",
    "## and the sqrt_TV radio interaction\n",
    "def get_feats(X):\n",
    "    processed_X = np.empty((np.shape(X)[0], 4))\n",
    "    \n",
    "    # get TV\n",
    "    processed_X[:,0] = X[:,0]\n",
    "    \n",
    "    # get sqrtTV\n",
    "    processed_X[:,1] = np.sqrt(X[:,0])\n",
    "    \n",
    "    # get radio\n",
    "    processed_X[:,2] = X[:,1]\n",
    "    \n",
    "    # get interaction\n",
    "    processed_X[:,3] = processed_X[:,1]*processed_X[:,2]\n",
    "    \n",
    "    return processed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FunctionTransformer is also stored in sklearn.preprocessing\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create A FunctionTransformer with the get_feats function\n",
    "## we defined above\n",
    "function = FunctionTransformer(get_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can transform our data with .transform\n",
    "## We'll demonstrate with the first 10 rows of the data\n",
    "print(\"[\\t TV, \\t sqrt_TV, \\t radio, \\t sqrtTV_radio]\")\n",
    "\n",
    "## Look no fit here, I wonder why?\n",
    "function.transform(X_train[1:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "Up to this point the only data preprocessing we've done is different types of transformations.\n",
    "\n",
    "However, in the future we'll encounter data that requires more work. `sklearn` offers a nice way to package everything you'd like to do to your data with one line of code.\n",
    "\n",
    "We can think of the data process like a pipeline, we input the data at one end and output the prediction on the other. Along the way it may need to be cleaned, scaled, or transformed, and then finally used to fit a model and the prediction is spit out on the other side. The `sklearn` `Pipeline` object allows us to get a \"pipe\" ready for our data to flow through.\n",
    "\n",
    "Let's again look at the `Advertising` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pipeline is stored in pipeline\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Call Pipeline\n",
    "## The steps are put in the order you want them executed\n",
    "## They are stored in a list of arrays\n",
    "## First we want to apply our get_feats function, we'll call this 'preprocess'\n",
    "## Then we want to fit a regression model, we'll call this 'reg'\n",
    "pipe = Pipeline([('preprocess',FunctionTransformer(get_feats)),\n",
    "                ('reg',LinearRegression(copy_X = True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we call the pipeline just like a regression object\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can access the regression coefficients like so\n",
    "## each part of the pipe is stored like a dictionary\n",
    "print(pipe['reg'].coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can make a prediction using .predict\n",
    "## just like with a normal regression object\n",
    "pipe.predict(X_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pipeline`s are quite useful, especially when we have additional preprocessing steps. They also make the process of measuring testing error much quicker to implement because we don't have to make transformed data versions of the test data by hand like before. That's what the `Pipeline` is for!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You Code\n",
    "\n",
    "You'll now make a pipeline to preprocess the data set from the interview problem we looked at in Notebook 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"PredictiveModelingAssessmentData.csv\")\n",
    "\n",
    "df_train = df.copy().sample(frac=.75,random_state = 440)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(df_train, figsize=(14,14))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a pipeline that takes in an array containing columns for $x_1$ and $x_2$ and fits the following regression model:\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\beta_3 \\log(x_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Turn the training data into an input array, X_train\n",
    "## and an output array, y_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a custom function to preprocess the input data\n",
    "## for the desired model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the pipeline here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the pipeline here, then look at the \n",
    "## regression coefficients\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fit` vs. `fit_transform` vs. `transform`\n",
    "\n",
    "As I've alluded to in the code snippets there are subtle differences between `fit`, `fit_transform` and `transform`. I'm sure that some of you have wondered about those differences, and when to use each one. These are all standard <i>methods</i> of `sklearn` `Transformer` objects.\n",
    "\n",
    "#### `fit`\n",
    "\n",
    "Some `Transformer`s can't transform your data without first knowing something about the data. For example with the `PolynomialFeatures` `Transformer` it needs to know the number of columns in your data set before it can decide what powers of columns to return. The `fit` method sends your input data through the `Transformer` and fits whatever needs to be fit in order for the `Transformer` can transform the data.\n",
    "\n",
    "#### `transform`\n",
    "\n",
    "This built-in method takes your input data and actually transforms it. <i>Note that this can only be called after the Transformer has been fit</i>. In our `PolynomialFeatures` example this means taking in the columns and squaring, cubing, and raising them to whatever power was specified.\n",
    "\n",
    "#### `fit_transform`\n",
    "\n",
    "This built-in method performs both of the prior steps at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's It!\n",
    "\n",
    "That's it for this notebook. We'll return to pipelines at a later time so we can learn more advanced pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2021.\n",
    "\n",
    "Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
