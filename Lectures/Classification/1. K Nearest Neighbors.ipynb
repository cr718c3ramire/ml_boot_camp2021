{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $k$-Nearest Neighbors\n",
    "\n",
    "In this notebook we'll introduce our first classification algorithm, $k$-nearest neighbors.\n",
    "\n",
    "## What You'll Accomplish\n",
    "\n",
    "- We'll see our first classification example,\n",
    "- You'll work through a simple algorithm, $k$-nearest neighbors,\n",
    "- We'll introduce our first classification performance measure, accuracy.\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the packages we'll need\n",
    "\n",
    "## to get the iris data\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "## for data handling \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(iris['data'],columns = ['sepal_length','sepal_width','petal_length','petal_width'])\n",
    "iris_df['iris_class'] = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This chunk of code is going to plot the data\n",
    "sns.lmplot(data = iris_df, x = 'sepal_length', \n",
    "            y = 'sepal_width',hue = 'iris_class',fit_reg=False,\n",
    "            height = 6.5,legend=False)\n",
    "\n",
    "plt.legend(title='Iris Class', loc='upper left', \n",
    "           labels=['setosa', 'versicolor', 'virginica'], \n",
    "           fontsize = 12)\n",
    "plt.xlabel(\"Sepal Length\",fontsize = 16)\n",
    "plt.ylabel(\"Sepal Width\",fontsize = 16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our iris data has three distinct classes, we can imagine a world in which we'd want to build a model that takes in `petal_width`, `petal_length`, `sepal_width`, and `sepal_length` then predicts what kind of iris we have.\n",
    "\n",
    "People build entire business models around classification problems. For example, <a href=\"https://www.covermymeds.com/main/\">CoverMyMeds</a> started by solving the problem \"How can I predict whether or not my prescription will need a prior authorization form?\". <a href=\"https://www.upstart.com/\">Upstart</a> tries to predict whether or not someone will be a good candidate for their loans.\n",
    "\n",
    "These problems are ubiquitous in our every day lives. Now lets start learning how we can use supervised learning techniques to solve them.\n",
    "\n",
    "## A Simple Algorithm\n",
    "\n",
    "We'll now work through a classification problem with one of the simplest classification algorithms, $k$-nearest neighbors.\n",
    "\n",
    "### KNN\n",
    "\n",
    "$k$-nearest neighbors is quite straightforward. When you want to classify an unlabeled point, you find the $k$ closest other data points in the data space. Whichever class is most present among the $k$ neighbors is what the algorithm classifies the unlabeled point as. In the case of ties, the class is randomly assigned from the tied classes.\n",
    "\n",
    "Let's look at a picture to see what we mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here is some random data\n",
    "## to illustrate the knn concept\n",
    "np.random.seed(440)\n",
    "xs = np.random.randn(50,2)\n",
    "os = np.random.randn(50,2)-np.array([3,0])\n",
    "\n",
    "unlabeled = [-1.2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We now plot that data\n",
    "plt.figure(figsize = (12,9))\n",
    "\n",
    "plt.plot(xs[:,0],xs[:,1],'rx',label = \"X\",markersize=8)\n",
    "plt.plot(os[:,0],os[:,1],'bo',label = \"O\",markersize=8)\n",
    "plt.plot(unlabeled[0],unlabeled[1],'gv',label = \"Unknown\",markersize=12)\n",
    "plt.xlabel(\"Feature 1\", fontsize = 16)\n",
    "plt.ylabel(\"Feature 2\", fontsize = 16)\n",
    "plt.legend(fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plot above, how would would knn classify the unlabeled point for $k=1$, $k=5$, $k=10$?\n",
    "\n",
    "### Stratified Train Test Splits\n",
    "\n",
    "Before returning to our iris data set let's take a brief aside on training test splits for classification problems. Consider the following phony data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(10,4)\n",
    "y = np.zeros(10)\n",
    "y[2] = 1\n",
    "y[7] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform a train test split like we did in our regression notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train_test_split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First input the features, then the target\n",
    "# specify what fraction of your data you want to test\n",
    "# then set a random_state\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.4, \n",
    "                                                    random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notice anything?\n",
    "\n",
    "This can be avoided by performing a <i>stratified train-test split</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.4, \n",
    "                                                    random_state=111,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratified splits first separate the data according to class. Each class is then randomly separated in two. Let's illustrate this with a picture.\n",
    "\n",
    "<img src=\"stratify.png\" style=\"width:80%\"></img>\n",
    "\n",
    "\n",
    "\n",
    "Now our particular data set above was an extreme example, however this sort of thing can be an issue in a number of classification problems of interest where the desired class is a rare occurence. One such example would be detecting cases of credit card fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an iris classifier\n",
    "\n",
    "We'll now demonstrate the flow of a classification problem, by using knn to build an iris classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can turn the data in numpy arrays\n",
    "## like so\n",
    "X = iris_df[['sepal_length','sepal_width','petal_length','petal_width']].to_numpy()\n",
    "y = iris_df['iris_class'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's try coding along!\n",
    "## You write code for the train test split here\n",
    "## I'll wait for a minute before typing the answer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state = 614, \n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the Model\n",
    "\n",
    "Now we can fit the model to our train data, let's use $k=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Just like with regression we'll import from\n",
    "## sklearn\n",
    "## import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The docs for `KNeighborsClassifier` <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\">https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the model\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the model\n",
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measures - Accuracy \n",
    "\n",
    "One way to measure how well our model did is to calculate its <i>accuracy</i>. Accuracy is the number of correct predictions divided by the number of total predictions we made. Let's see how well our model does, on our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a prediction on our train set\n",
    "y_predict = knn.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We calculate the accuracy here\n",
    "## sum a list of booleans and True gets cast as 1\n",
    "## False gets cast as 0\n",
    "print(\"Our model has a \",\n",
    "      np.round(sum(y_predict == y_train)/len(y_train)*100,2),\n",
    "      \"% accuracy on the training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not too bad!\n",
    "\n",
    "What could we do to the $k$-nearest neighbors model to change our accuracy? \n",
    "\n",
    "Also, is there anything else we could do to get a better idea of the generalization error? \n",
    "\n",
    "### Cross Validation for Model Assessment\n",
    "\n",
    "Just as we did for some of our regression models we can assess multiple models at once and compare the average accuracies of them all to choose the best model.\n",
    "\n",
    "### You Code\n",
    "\n",
    "Implement CV with $5$ splits. Set a random state so you could recreate your split. Going from $1$ to $20$ neighbors find the model that has the best CV accuracy.\n",
    "\n",
    "I'll get you set up with the cross validation using a new python function `StratifiedKFold`.\n",
    "\n",
    "Remember finish as much as you can in our allotted time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import StratifiedKFold\n",
    "## THIS IS NEW!!!\n",
    "## This allows you to do stratified k fold cross validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a kfold model object, with a random_state\n",
    "## Note that we use StratifiedKFold just like KFold\n",
    "kfold = StratifiedKFold(5,shuffle = True,random_state = 440)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You'll use this function for calculating the accuracy\n",
    "## Just input the model, the input data and the target data\n",
    "def get_acc(model,X,y):\n",
    "    pred = model.predict(X)\n",
    "    return np.sum(pred == y)/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You run 5 fold cross validation here\n",
    "\n",
    "## Make an empty array to hold your cv accuracies here\n",
    "\n",
    "\n",
    "## Loop through all the possible neighbors from 1 to max_neighbors\n",
    "max_neighbors = 20\n",
    "\n",
    "\n",
    "## Perform the cross validation loop here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot how the accuracy changes\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "## Plot the number of neighbors on the x\n",
    "## plot the avg cross validation accuracy on the y\n",
    "plt.plot()\n",
    "\n",
    "## Use these as your axes labels\n",
    "plt.xlabel(\"Number of Neighbors\", fontsize=16)\n",
    "plt.ylabel(\"Average CV Accuracy (%)\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot it looks like the \"best\" model here is going to be which one? \n",
    "\n",
    "Let's go ahead and calculate accuracy on our test set using this best model. Do that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's it!\n",
    "\n",
    "That's it for this notebook! Up next we'll review some of those other classification performance measures we mentioned in the Introduction notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2021.\n",
    "\n",
    "Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
