{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Classification Performance Measures\n",
    "\n",
    "So far we've been introduced to the concept of classification and we've learned a basic algorithm, $k$-nearest neighbors.\n",
    "\n",
    "In this notebook we'll discuss additional performance measures for classification problems.\n",
    "\n",
    "## What We'll Accomplish\n",
    "\n",
    "- Introduce the confusion matrix\n",
    "- Discuss precision, recall, the false positive rate, the true positive rate\n",
    "- Look at ROC curves and the area beneath them\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the iris data\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# for data handling \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Isn't Everything\n",
    "\n",
    "Sometimes accuracy is a misleading measure. Suppose your dataset has an extreme split, say $90\\%$ in class $0$ and $10\\%$ in class $1$. In that instance we could build a $90\\%$ accurate classifier by just labeling everything as $0$. However, we didn't correctly identify any of the class $1$ objects. This would be awful, if for instance class $1$ represented a person that has a disease.\n",
    "\n",
    "It is thus important to consider other performance measures when deciding if a classifier is good.\n",
    "\n",
    "### Now Things Start To Get Confusing\n",
    "\n",
    "Additional performance measures are derived from the confusion matrix, pictured for binary problems below.\n",
    "\n",
    "<img src=\"conf_mat.png\" alt=\"Confusion Matrix Image\" style=\"width:50%px;\">\n",
    "\n",
    "Here the diagonal of the box represents data points that were correctly predicted by the algorithm, the off-diagonal represents points that are incorrectly predicted by the algorithm. Contained within each box of the confusion matrix are counts of how the algorithm sorted. For instance, in the TP box would be the total number of correct positive (correctly classified as $1$) classifications the algorithm made. (<i>Note that you can extend the confusion matrix to a multiclass problem by just adding rows and columns accordingly. However, we'll lost the true positive true negative nomenclature</i>.)\n",
    "\n",
    "Two popular measures derived from the confusion matrix are the algorithm's <i>precision</i> and <i>recall</i>:\n",
    "\n",
    "$$\n",
    "\\text{precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}, \\text{ out of all points predicted to be class } 1, \\text{ what fraction were actually class } 1.\n",
    "$$\n",
    "$$\n",
    "\\text{recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}, \\text{ out of all the actual data points in class } 1 \\text{, what fraction did the algorithm correctly predict?}\n",
    "$$\n",
    "\n",
    "You can think of precision as how much you should trust the algorithm when it says something is class $1$. Recall estimates the probability that the algorithm correctly detects class $1$ data points.\n",
    "\n",
    "You've likely heard of these types of measures in all the news stories about COVID-19 tests, as they are quite important in the field of public health.\n",
    "\n",
    "Let's examine the training precision and recall for a virginica classifier using the iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(iris['data'],columns = ['sepal_length','sepal_width','petal_length','petal_width'])\n",
    "\n",
    "## Create a virginica variable\n",
    "## this will be our target\n",
    "iris_df['virginica'] = 0 \n",
    "iris_df.loc[iris['target'] == 2,'virginica'] = 1\n",
    "\n",
    "X = iris_df[['sepal_length','sepal_width','petal_length','petal_width']].to_numpy()\n",
    "y = iris_df['virginica'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=111,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build a $k$-nearest neighbor classifier using $k=10$. We'll then examine the confusion matrix on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import Nearest Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the model object\n",
    "knn = KNeighborsClassifier(n_neighbors = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fit the model object\n",
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the predictions\n",
    "y_train_pred = knn.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now we can import the confusion matrix\n",
    "## function from sklearn\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix docs, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[73,  2],\n",
       "       [ 3, 34]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we have $73$ true negatives (TN), $2$ false positives (FP), $3$ false negatives (FN) and $34$ true positives (TP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You Code\n",
    "\n",
    "You write code to calculate the precision and recall for this classifier.\n",
    "\n",
    "#### Calculate the Recall and Precision by Hand\n",
    "\n",
    "Using the `sklearn` `confusion_matrix` function calculate the precision and recall by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print out the precision and recall here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use `sklearn` to calculate the precision and recall\n",
    "\n",
    "Look at the following documentations to figure out how you can us `sklearn` to calculate the recall or precision score.\n",
    "\n",
    "- `precision_score` docs, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html</a>\n",
    "- `recall_score` docs, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "## Import the functions from sklearn\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "## calculate the recall and precision here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ROC Curve\n",
    "\n",
    "Another way to measure the performance of your classifier is the <i>receiver operating characteristic</i> (ROC) curve.\n",
    "\n",
    "This plots the <i>true positive rate</i> (tpr), i.e. the recall, vs the <i>false positive rate</i> (fpr). Returning to the confusion matrix the fpr is:\n",
    "$$\n",
    "\\text{fpr} = \\frac{\\text{FP}}{\\text{FP}+\\text{TN}} = 1 - \\frac{\\text{TN}}{\\text{FP} + \\text{TN}} = 1 - \\text{specificity},\n",
    "$$\n",
    "specifity is a term used a lot in public health and is another name for true negative rate. Public Health so hot right now.\n",
    "\n",
    "Let's plot an ROC curve for our logistic regression model.\n",
    "\n",
    "#### Prediction Probabilities\n",
    "\n",
    "In order to get arrays of true positive and false positive rates for a particular classifier you need a vector of probabilities (the probability that each observation is the class of interest). You then calculate the true and false positive rates for various probability cutoffs.\n",
    "\n",
    "Let's see what we mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This gets the probabilities\n",
    "\n",
    "## Note this is common for most of sklearn's classification algorithms\n",
    "probs = knn.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The first column is the probability that the observation is 0\n",
    "## The second column is the probability that the observation is 1\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now lets calculate the TPR and FPR for a\n",
    "## cutoff of .4\n",
    "cutoff = .4\n",
    "\n",
    "y_train_pred = np.ones(len(y_train))\n",
    "y_train_pred[probs[:,1] < .4] = 0\n",
    "\n",
    "conf_mat = confusion_matrix(y_train, y_train_pred)\n",
    "tp = conf_mat[1,1]\n",
    "tn = conf_mat[0,0]\n",
    "fn = conf_mat[1,0]\n",
    "fp = conf_mat[0,1]\n",
    "\n",
    "print(\"The false positive rate is\",np.round(fp/(tn+fp),4)*100)\n",
    "print(\"The true positive rate is\",np.round(tp/(tp+fn),4)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build a loop to get the various tpr and fprs for different cutoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs = np.arange(0,1.01,.01)\n",
    "\n",
    "tprs = []\n",
    "fprs = []\n",
    "\n",
    "for cutoff in cutoffs:\n",
    "    y_pred = np.ones(len(probs))\n",
    "    y_pred[probs[:,1] < cutoff] = 0 \n",
    "\n",
    "    # tpr = tp/(tp + fn)\n",
    "    tpr = np.sum(y_pred[y_train == 1])/np.sum(y_train == 1)\n",
    "\n",
    "    # fpr = fp/(fp + tn)\n",
    "    fpr = np.sum(y_pred[y_train == 0])/np.sum(y_train==0)\n",
    "    \n",
    "    tprs.append(tpr)\n",
    "    fprs.append(fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.plot(fprs,tprs)\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\",fontsize=16)\n",
    "plt.ylabel(\"True Positive Rate\",fontsize=16)\n",
    "\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.title(\"ROC Curve\", fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You Code\n",
    "\n",
    "Go to the documentation for the `sklearn` function `roc_curve`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html</a> and attempt to figure out how you can use that instead of a for loop to plot the roc curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import roc_curve here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use roc_curve to get the fpr, tpr and cutoffs\n",
    "## for the training data and the corresponding probabilities\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the corresponding ROC Curve here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUC - (Do this in the breakout session)\n",
    "\n",
    "ROC curves come with an additional measure called AUC (area under the curve). An AUC of $1$ would be a perfect classifer, an AUC of $.5$ is what you'd get with random guessing (for a binary classifier). So what is a good AUC? Well it's hard to say with a single classifier, but it can be used to compare multiple classifiers, for example if you're choosing between a classifier with AUC $.8$ and an AUC of $.85$ you'd go with the one that has AUC $.85$.\n",
    "\n",
    "Let's see how to calculate AUC with `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import the function from sklearn\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`roc_auc_score` docs, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## put in the true classes along with the \n",
    "## probability scores.\n",
    "roc_auc_score(y_train,probs[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the training set AUC scores of $k$-nearest neighbors classifiers with $k=1$ all the way to $k=20$. Which one performs best on the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "## Write a loop for getting the aucs for different\n",
    "## values of k here\n",
    "for k in range(1,21):\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the training AUC score as a function of\n",
    "## the number of neighbors here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the roc_curve of the model with best\n",
    "## training AUC here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for this notebook!\n",
    "\n",
    "The important take away for this notebook is that we need to be careful about what performance metric we use for classification problems. Anytime you work on a classification project, think about what the end goal is for either yourself or your use case. That should inform what performance you prioritize.\n",
    "\n",
    "Next time we'll learn about logistic regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2021.\n",
    "\n",
    "Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
