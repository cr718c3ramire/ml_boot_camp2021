{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Just when we thought we were done with regression, it pulls us back in!\n",
    "\n",
    "To get an idea of how this algorithm works we'll look at some phony classification data. In a later notebook you'll use logistic regression to build a cancer classifier.\n",
    "\n",
    "## What You'll Accomplish\n",
    "\n",
    "- Learn the logistic regression algorithm\n",
    "- Show how you can interpret logistic regression output\n",
    "- Talk about classification cutoffs\n",
    "- Introduce the generalized linear model framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Algorithm\n",
    "\n",
    "We'll be using logistic regression for binary classification, classification problems with only two classes typically coded as $0$ or $1$. Normally the class denoted as $1$ is something we want to identify, for example someone that has a disease or someone that qualifies for a loan. \n",
    "\n",
    "<i>Note that logistic regression can be adapted for more than two classes, but for now we'll only focus on two.</i>\n",
    "\n",
    "### From Binary to Continuous\n",
    "<i>Logistic regression is a form of regression algorithm.</i> It turns out that this may be somewhat of a controversial statement... <a href=\"https://twitter.com/TenanATC/status/1386332061087277057?s=20\">https://twitter.com/TenanATC/status/1386332061087277057?s=20</a>, but it is important to remember that it is in fact a regression technique used to solve classification problems.\n",
    "\n",
    "Remember that regression algorithms are usually used to predict continuous outcomes, but binary classification is in no way continuous. This is where logisitic regression is a little clever, instead of modeling the output class, it models the probability that a particular data point is an instance of class $1$.\n",
    "\n",
    "Let's dive in with some fake data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the randomly generated data\n",
    "data = np.loadtxt(\"random_binary.csv\",delimiter = \",\")\n",
    "X = data[:,0]\n",
    "y = data[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform a stratified test train split\n",
    "## Practice, write the code to do that in these two blocks\n",
    "## First import the package\n",
    "\n",
    "## I'll wait before writing the code myself :)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now split the data\n",
    "## Have 20% for testing\n",
    "## Set 614 as the random state\n",
    "## and stratify the split\n",
    "\n",
    "## I'll wait before writing the code myself :)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,\n",
    "                                                test_size=.2,\n",
    "                                                shuffle=True,\n",
    "                                                random_state=614,\n",
    "                                                stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training data\n",
    "plt.figure(figsize = (10,8))\n",
    "\n",
    "plt.scatter(X_train,y_train)\n",
    "plt.ylim((-.1,1.1))\n",
    "plt.xlabel(\"Feature\",fontsize = 16)\n",
    "plt.ylabel(\"Class\",fontsize = 16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get down to the idea behind logistic regression. While the $y$-axis of the above plot says Class we could just as easily label it the Probability the instance is $1$. In this case since we know the class of each data point in the training set, the probability can only be $0$ or $1$. Now suppose you have a new data point for which you only have the vector of predictors, $X$. We're interested in the probability that this data point has class $y=1$, call this probability $P(y=1|X) = p(X)$. $p(X)$ can take on all values in $[0,1]$, this is a continuous variable.\n",
    "\n",
    "The way we model the probability in logistic regression is with a sigmoidal curve, the general form looks like this:\n",
    "$$\n",
    "f(x) = \\frac{1}{1+e^{-x}}.\n",
    "$$\n",
    "A graph of the curve this function produces is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 10\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "plt.plot(np.arange(-x,x,.01),1/(1+np.exp(-np.arange(-x,x,.01))))\n",
    "\n",
    "\n",
    "plt.xlabel(\"x\",fontsize = 14)\n",
    "plt.ylabel(\"f(x)\",fontsize = 14)\n",
    "\n",
    "plt.title(\"A Sigmoidal Curve\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this function stays between $0$ and $1$. Also like our phony data data it transitions from class $0$, to class $1$ in a continuous manner. This is the function type we'd like to use as our model.\n",
    "\n",
    "The model that is used in logistic regression is:\n",
    "$$\n",
    "p(X) = \\frac{1}{1 + e^{-X\\beta}},\n",
    "$$\n",
    "where $\\beta = \\left(\\beta_0,\\beta_1,\\dots,\\beta_m\\right)^T$ is a column vector of coefficients, $X$ has been extended to include a column of ones.\n",
    "\n",
    "The model is fit using the statistical method of maximum likelihood estimators (for a derivation of the loss function see the homework on logistic regression).\n",
    "\n",
    "Let's see how to use `sklearn` to fit a logistic regression model to our phony data, then see how we can use it for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You Code \n",
    "\n",
    "You'll make and fit the Logistic Regression model below for our phony data.\n",
    "\n",
    "The procedure follows the standard `sklearn` pattern for fitting and predicting a model. If you need more help check out the docs, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html</a>. The model object is stored in `sklearn.linear_model` and is called `LogisticRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import the logistic regression method\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a model instance here\n",
    "## call it log_reg\n",
    "log_reg = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the model here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've fit the model to our training data, let's plot the model along with our train data, and see what we're talking about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the training data along with the fitted model\n",
    "\n",
    "## use this to plot you fitted model\n",
    "xs = np.linspace(0,1,1000)\n",
    "pred = log_reg.predict_proba(xs.reshape(-1,1))[:,1]\n",
    "\n",
    "# Plot figure here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dotted red line shows how the logistic regression model fit on our training data changes the probability of being and instance of the $1$ class as we increase the value for our feature. \n",
    "\n",
    "### From Probabilities to Classifications\n",
    "\n",
    "How would you propose we turn probability output into classifications?\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "The standard approach is to just choose a probability cutoff, for instance if $p(X) \\geq .5$ we classify the instance as $1$, otherwise we say it is a $0$. This is an example of a <i>decision boundary</i>, any point to the left of the boundary gets classified as a $0$, on the right a $1$. Decision boundaries are a big part of many classification algorithms, so this won't be the last time we see them.\n",
    "\n",
    "Let's see what our training accuracy is for a cutoff of your choice. Someone enter a cutoff value into the Zoom chat now :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write code to calculate the accuracy for any cutoff, then choose your cutoff\n",
    "cutoff = .5\n",
    "\n",
    "## store the predicted probabilities\n",
    "y_prob = log_reg.predict_proba(X_train.reshape(-1,1))[:,1]\n",
    "\n",
    "## assign the value based on the cutoff\n",
    "y_train_pred = 1*(y_prob > cutoff)\n",
    "\n",
    "## print the accuracy\n",
    "## input the accuracy after \"is\",\n",
    "print(\"The training accuracy for a cutoff of\",cutoff,\n",
    "      \"is\", np.sum(y_train_pred == y_train)/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now plot how the accuracy changes with the cutoff\n",
    "cutoffs = np.arange(0,1.01,.01)\n",
    "accs = []\n",
    "\n",
    "for cutoff in cutoffs:\n",
    "    y_train_pred = 1*(y_prob > cutoff)\n",
    "    accs.append(np.sum(y_train_pred == y_train)/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.scatter(cutoffs,accs)\n",
    "\n",
    "plt.xlabel(\"Cutoff\",fontsize=16)\n",
    "plt.ylabel(\"Training Accuracy\",fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Logistic Regression\n",
    "\n",
    "One nice thing about this algorithm is that we can interpret the results. This is always a nice feature of an algorithm.\n",
    "\n",
    "Reconsider the statistical model that we fit:\n",
    "$$\n",
    "p(X) = \\frac{1}{1 + e^{- X \\beta}}.\n",
    "$$\n",
    "Rearranging this equation we find the following:\n",
    "$$\n",
    "\\log\\left(\\frac{p(X)}{1-p(X)}\\right) =  X \\beta.\n",
    "$$\n",
    "The expression $p(X)/(1-p(X))$ is known as the odds of the event $y=1$. So the statistical model for logistic regression is really just a linear model for the $\\log$ odds of being class $1$. This allows us to interpret the coefficients of our model.\n",
    "\n",
    "Look at the model we just fit:\n",
    "$$\n",
    "\\log\\left(\\frac{p(x)}{1-p(x)}\\right) = \\beta_0 + \\beta_1 x, \\text{ or } \\text{Odds}|x = C e^{\\beta_1 x}\n",
    "$$\n",
    "where $x$ is the feature, and $C$ is some constant we don't care about. \n",
    "\n",
    "So if we increase $x$ from say $d$ to $d+1$, a $1$ unit increase, then our odds are $e^{\\beta_1}$ units times larger (or smaller depending on the value of $\\beta_1$), we can see this below:\n",
    "$$\n",
    "\\frac{\\text{Odds}|x = d+1}{\\text{Odds}|x=d} = \\frac{e^{\\beta_1 (d+1)}}{e^{\\beta_1 d}} = e^{\\beta_1}\n",
    "$$\n",
    "\n",
    "Let's look at the coefficient from our phony data logistic regression and interpret it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"A .1 unit increase in our feature multiplies\" + \n",
    "      \" the odds of being classified as 1 by \" + \n",
    "      str(np.round(np.exp(.1*log_reg.coef_[0][0]),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Assumptions\n",
    "\n",
    "While we were explaining the concept of logistic regression, we didn't mention any of the assumptions of the algorithm. Let's talk about that here before we move on to real data.\n",
    "<ol>\n",
    "    <li>Each sample must be independent from all other samples,</li>\n",
    "    <li>When using multiple predictors, they shouldn't be correlated,</li>\n",
    "    <li>The log odds depend linearly on the predictors,</li>\n",
    "    <li>Logistic regression should have a largish data set to work with.</li>\n",
    "</ol>\n",
    "\n",
    "We did not worry about these assumptions in this notebook because the data were randomly generated to fit these assumptions. However, in real world applications you should check them when deciding whether or not logistic regression is a good model choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Linear Models\n",
    "\n",
    "Before we end this notebook I want to briefly touch on a more general modeling framework that captures both linear and logistic regression, <i>generalized linear models</i>.\n",
    "\n",
    "Let's review the two types of regression models we've discussed.\n",
    "\n",
    "#### Linear Regression\n",
    "\n",
    "For a continuous target, $y$, and a features matrix, $X$, we had:\n",
    "$$\n",
    "E(y|X) = X\\beta.\n",
    "$$\n",
    "\n",
    "#### Logistic Regression\n",
    "\n",
    "For a binary target, $y$, and a feature matrix, $X$, we had:\n",
    "$$\n",
    "\\log\\left( \\frac{P(y=1|X)}{1-P(y=1|X)} \\right) = X\\beta.\n",
    "$$\n",
    "\n",
    "Where we should note that for a binary $0$-$1$ variable $P(y=1|X) = E(y|X)$ so in reality we had:\n",
    "$$\n",
    "\\log\\left( \\frac{E(y|X)}{1-E(y|X)} \\right) = X\\beta.\n",
    "$$\n",
    "\n",
    "#### Notice Anything?\n",
    "\n",
    "In both cases we could write the following:\n",
    "$$\n",
    "g(E(y|X)) = X\\beta,\n",
    "$$\n",
    "\n",
    "where we made a specific choice for the functional form of $g$ depending on the data type of $y$. This is the idea behind generalized linear models.\n",
    "\n",
    "### Three Components\n",
    "\n",
    "Given features, $X$, and target, $y$, a generalized linear model relating $y$ to $X$ is composed of three components. \n",
    "\n",
    "##### 1.  Random Component\n",
    "\n",
    "This is where you assume a probability distribution for $y|X$. It is typically assumed that distribution for $y|X$ comes from the <i>exponential family</i>, <a href=\"https://en.wikipedia.org/wiki/Exponential_family\">https://en.wikipedia.org/wiki/Exponential_family</a>.\n",
    "\n",
    "##### 2. Systematic Component\n",
    "\n",
    "Where you relate the parameters $\\beta$ to the features $X$. It is always the case in a generalized linear model that the systematic component is $X\\beta$.\n",
    "\n",
    "##### 3. Link Component\n",
    "\n",
    "The connection between the random and systematic components.\n",
    "\n",
    "Combining all three of these components gives the following:\n",
    "$$\n",
    "g(E(y|X)) = X\\beta.\n",
    "$$\n",
    "\n",
    "We won't do anything else with generalized linear models in this program or in python. However, as you continue on in your own data science work it may be useful to be familiar with the generalized linear model setup. For those interested in learning more I encourage you to check out the following resources:\n",
    "\n",
    "<a href=\"http://www.stat.cmu.edu/~ryantibs/advmethods/notes/glm.pdf\">http://www.stat.cmu.edu/~ryantibs/advmethods/notes/glm.pdf</a>\n",
    "\n",
    "<a href=\"http://www.utstat.toronto.edu/~brunner/oldclass/2201s11/readings/glmbook.pdf\">http://www.utstat.toronto.edu/~brunner/oldclass/2201s11/readings/glmbook.pdf</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's it!\n",
    "\n",
    "That's it for this notebook. Up next you'll use logistic regression to classify cancer cases!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2021.\n",
    "\n",
    "Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
